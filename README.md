# Deep-Learning-in-NLP



## Lecture 1

* Introduction

### Homework 1

* MLP and Character Language Modeling

## Lecture 2

* Basic Word Embedding
* CBOW Model
* Skip-Gram Model
* Doc2Vec
* PyTorch Example

### Homework 2

* CBOW

## Lecture 3

* Glove
* Fast Text
* Skip Gram with Negative Sampling is Matrix Factorization

### Homework 3

* Text Classification with GloVe

## Lecture 4

* Neural Networks
* Optimizations - Theory + Examples
* BackProp - Theory + Examples
* Residual Connections
* BatchNorm and LayerNorm
* Regularization - Dropout
* Wave Net

### Homework 4

* Skip Gram as Matrix Factorization

## Lecture 5

* Convolutional Neural Networks
* CNN for NLP
* Example: Text Classification
* Example: Language Modeling with Gated CNN

### Homework 5

* WaveNet LM

## Lecture 6

* Language Modeling
* Recurrent Neural Network
* LSTM
* Decoding

### Homework 6

* CNN - Text Classification

## Lecture 7

* NMT
* Attention

### Homework 7

* Char RNN and Random Decoding

## Lecture 8

* Contextual Embedding and Applications of Attention
* Character-Aware NLP
* BiDAF
* CoVe
* ELMo
* ULM-Fit

### Homework 8

* Char RNN and Beam Search

## Lecture 9

* The Transformer
* GPT1

### Homework 9

* Seq2Seq MT

## Lecture 10

* BERT
* ROBERTA
* Knowledge Distillation
* Distil BERT
* Sentence BERT

### Homework 10

* Seq2Seq MT with Attention

## Lecture 11

* Natural Language Degeneration
* GPT2
* GPT3
* SpanBERT
* BART
* ALBERT

### Homework 11

* Written Homework

## Lecture 12

* Course Review

### Homework 12

* NER with RNN at the World and Char Level

## Lecture 13

* Adapers
* Prefix - Tuning
* LORA

### Homework 13

* GPT

