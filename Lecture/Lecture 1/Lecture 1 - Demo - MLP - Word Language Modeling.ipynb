{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5db00378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer, ngrams_iterator\n",
    "from torchtext.datasets import DATASETS\n",
    "from torchtext.prototype.transforms import load_sp_model, PRETRAINED_SP_MODEL, SentencePieceTokenizer\n",
    "from torchtext.utils import download_from_url\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torchviz import make_dot\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46df7f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcb35e9c",
   "metadata": {},
   "source": [
    "### Information\n",
    "- torchtext repo: https://github.com/pytorch/text/tree/main/torchtext\n",
    "- torchtext documentation: https://pytorch.org/text/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c4d6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9140e3c",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9ace94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"WikiText2\"\n",
    "DATA_DIR = \".data\"\n",
    "DEVICE = \"mps\"\n",
    "LR = 4.0\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 5\n",
    "MIN_FREQUENCY = 5\n",
    "PADDING_VALUE = 0\n",
    "PADDING_IDX = PADDING_VALUE\n",
    "\n",
    "# n-gram level\n",
    "n = 3\n",
    "# Hidden layer dimension\n",
    "h = 100\n",
    "# Word embedding dimension\n",
    "m = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff816ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09f16039",
   "metadata": {},
   "source": [
    "### Get the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f471ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_english_tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1b61bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'some', 'text', '.', '.', '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_english_tokenizer(\"This is some text ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68a50055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed later.\n",
    "TOKENIZER = basic_english_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8620a436",
   "metadata": {},
   "source": [
    "### Get the data and get the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c84225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield TOKENIZER(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "affa3375",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = DATASETS[DATASET](root=DATA_DIR, split=\"train\")\n",
    "VOCAB = build_vocab_from_iterator(yield_tokens(train_iter), min_freq = MIN_FREQUENCY, specials=['<unk>'])\n",
    "\n",
    "# Set the default index to 1. Otherwise, VOCAB['unknownbigword'] will raise an Exception.\n",
    "VOCAB.set_default_index(VOCAB['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62336be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "518918c0",
   "metadata": {},
   "source": [
    "Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de48bde8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 324, 0, 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB['yoyooyoyoy'], VOCAB['house'], VOCAB['<pad>'], VOCAB['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24247d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20409\n"
     ]
    }
   ],
   "source": [
    "print(len(VOCAB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "574cce1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[324, 324, 1374, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB.lookup_indices(TOKENIZER(\"House house houses ThisisnotaKNownWord\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be1e272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df651a44",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94741f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(x):\n",
    "    return VOCAB(TOKENIZER(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e903610",
   "metadata": {},
   "source": [
    "Nice link on collate_fn and DataLoader in PyTorch: https://python.plainenglish.io/understanding-collate-fn-in-pytorch-f9d1742647d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95311731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    source_list, target_list = [], []\n",
    "        \n",
    "    for sentence in batch:\n",
    "                        \n",
    "        tokens = text_pipeline(sentence)\n",
    "        \n",
    "        for i in range(len(tokens)):\n",
    "            if i + n -1 <= len(tokens) - 1:\n",
    "                source, target = tokens[i:i+n-1], tokens[i+n-1]\n",
    "                source_list.append(torch.tensor(source))\n",
    "                target_list.append(torch.tensor(target))\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "    source_list = torch.vstack(source_list) if source_list else torch.empty()\n",
    "    target_list = torch.vstack(target_list) if target_list else torch.empty()\n",
    "            \n",
    "    return source_list.to(DEVICE), target_list.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca51b36",
   "metadata": {},
   "source": [
    "### Set up the model\n",
    "- nn.Embedding(V, D) is like a hash map.\n",
    "- It takes in data generally of the shape N X T where N is the batch size and returns a tensor of shape N X T X D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b4f143f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10])\n"
     ]
    }
   ],
   "source": [
    "# Data is of size 16 by 10 with a vocabulary of size 10.\n",
    "# Imagine that each token / word has a mapping of the sort {word -> id}.\n",
    "x = torch.randint(0, 10, (16, 10))\n",
    "e = nn.Embedding(10, 5)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8de85cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1fb50a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5317, -0.2339,  0.5199,  0.0933,  0.5804],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e(torch.tensor(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc2bd673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(3), num_classes=10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc52d408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5317, -0.2339,  0.5199,  0.0933,  0.5804],\n",
       "       grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(3), num_classes=10).float() @ e.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c0fa6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10, 5])\n"
     ]
    }
   ],
   "source": [
    "e = nn.Embedding(10, 5)\n",
    "# N X T X D - PyTorch is smart enough to realize you are passing in a batch.\n",
    "print(e(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8612ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ab8cb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the first Neural language models!\n",
    "class NeuralLanguageModel(nn.Module):\n",
    "    def __init__(self, V, m, h, n):\n",
    "        super(NeuralLanguageModel, self).__init__()\n",
    "        \n",
    "        # Vocabulary size.\n",
    "        self.V = V\n",
    "        \n",
    "        # Embedding dimension, per word.\n",
    "        self.m = m\n",
    "        \n",
    "        # Hidden dimension.\n",
    "        self.h = h\n",
    "        \n",
    "        # n in \"n-gram\".\n",
    "        self.n = n\n",
    "        \n",
    "        # Can you change all this stuff to use nn.Linear?\n",
    "        # Can also use nn.Parameter(torch.zeros(V, m)) for self.C but then we need one-hot and this is slow.\n",
    "        self.C = nn.Embedding(V, m)\n",
    "        \n",
    "        # nn.Linear((n-1) * m, h, bias=False) would give the same thing for the first one below, and similarly later. \n",
    "        self.H = nn.Parameter(torch.zeros((n-1) * m, h))\n",
    "        self.W = nn.Parameter(torch.zeros((n-1) * m, V))\n",
    "        self.U = nn.Parameter(torch.zeros(h, V))\n",
    "        \n",
    "        self.b = torch.nn.Parameter(torch.ones(V))\n",
    "        self.d = torch.nn.Parameter(torch.ones(h))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x is initially of dimension N X n-1 since batch is size N and context is of size n-1.\n",
    "        \n",
    "        # N X (n-1) X m \n",
    "        x = self.C(x)\n",
    "        \n",
    "        # N\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # N X (n-1) * m\n",
    "        x = x.view(B, -1)\n",
    "    \n",
    "        # N X V\n",
    "        y = self.b + torch.matmul(x, self.W) + torch.matmul(nn.Tanh()(self.d + torch.matmul(x, self.H)), self.U)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43d569e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "899045e6",
   "metadata": {},
   "source": [
    "### Set up the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02b8e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
    "model = NeuralLanguageModel(len(VOCAB), m, h, n).to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10508c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8994ae19",
   "metadata": {},
   "source": [
    "### Set up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eaaa82a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = DATASETS[DATASET](root=DATA_DIR, split=\"train\")\n",
    "test_iter = DATASETS[DATASET](root=DATA_DIR, split=\"test\")\n",
    "\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5af374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72b5bb91",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d58cc1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    total_loss, total_batches = 0.0, 0.0\n",
    "    log_interval = 100\n",
    "\n",
    "    for idx, (x, y) in tqdm(enumerate(dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if x.nelement() == 0:\n",
    "            continue\n",
    "        \n",
    "        logits = model(x)\n",
    "                        \n",
    "        # Get the loss.\n",
    "        loss = criterion(input=logits, target=y.squeeze(-1))\n",
    "\n",
    "        # Do back propagation.\n",
    "        loss.backward()\n",
    "                        \n",
    "        # Clip the gradients.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        \n",
    "        # Do an optimization step.\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_batches += 1\n",
    "                \n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            perplexity = torch.exp(torch.tensor(total_loss / total_batches)).item()\n",
    "            print(\n",
    "                \"| epoch {:3d} \"\n",
    "                \"| {:5d}/{:5d} batches \"\n",
    "                \"| perplexity {:8.3f} \"\n",
    "                \"| loss {:8.3f} \"\n",
    "                .format(\n",
    "                    epoch,\n",
    "                    idx,\n",
    "                    len(dataloader),\n",
    "                    perplexity,\n",
    "                    total_loss / total_batches,\n",
    "                )\n",
    "            )\n",
    "            total_loss, total_batches = 0.0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85722617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_batches = 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (x, y) in enumerate(dataloader):\n",
    "            logits = model(x)\n",
    "            total_loss += criterion(input=logits, target=y.squeeze(-1)).item()\n",
    "            total_batches += 1\n",
    "    return total_loss / total_batches, torch.exp(torch.tensor(total_loss / total_batches)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21ba24f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:12,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   100/ 2181 batches | perplexity 1355.038 | loss    7.212 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:30,  6.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2181 batches | perplexity  821.589 | loss    6.711 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [00:54,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   300/ 2181 batches | perplexity  710.921 | loss    6.567 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "403it [01:22,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   400/ 2181 batches | perplexity  658.290 | loss    6.490 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [01:54,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 2181 batches | perplexity  627.470 | loss    6.442 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [02:31,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   600/ 2181 batches | perplexity  611.853 | loss    6.416 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "701it [03:16,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   700/ 2181 batches | perplexity  585.083 | loss    6.372 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "801it [03:59,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   800/ 2181 batches | perplexity  554.135 | loss    6.317 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "901it [04:36,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   900/ 2181 batches | perplexity  557.891 | loss    6.324 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1002it [05:17,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  1000/ 2181 batches | perplexity  560.686 | loss    6.329 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1102it [05:53,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  1100/ 2181 batches | perplexity  536.561 | loss    6.285 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1201it [06:32,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  1200/ 2181 batches | perplexity  513.865 | loss    6.242 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1302it [07:17,  7.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  1300/ 2181 batches | perplexity  515.425 | loss    6.245 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1402it [07:58,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  1400/ 2181 batches | perplexity  490.106 | loss    6.195 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1502it [08:34,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  1500/ 2181 batches | perplexity  488.864 | loss    6.192 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1578it [09:03,  2.90it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, NUM_EPOCHS \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m     epoch_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m     train(train_dataloader, model, optimizer, criterion, epoch)\n\u001b[1;32m      4\u001b[0m     loss_val, perplexity_val \u001b[39m=\u001b[39m evaluate(valid_dataloader, model, criterion)\n\u001b[1;32m      5\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn [21], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, optimizer, criterion, epoch)\u001b[0m\n\u001b[1;32m      3\u001b[0m total_loss, total_batches \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m, \u001b[39m0.0\u001b[39m\n\u001b[1;32m      4\u001b[0m log_interval \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfor\u001b[39;00m idx, (x, y) \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(dataloader)):\n\u001b[1;32m      7\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m     \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mnelement() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "Cell \u001b[0;32mIn [12], line 11\u001b[0m, in \u001b[0;36mcollate_batch\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m+\u001b[39m n \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tokens) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     10\u001b[0m     source, target \u001b[39m=\u001b[39m tokens[i:i\u001b[39m+\u001b[39mn\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], tokens[i\u001b[39m+\u001b[39mn\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 11\u001b[0m     source_list\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39;49mtensor(source))\n\u001b[1;32m     12\u001b[0m     target_list\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mtensor(target))\n\u001b[1;32m     13\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader, model, optimizer, criterion, epoch)\n",
    "    loss_val, perplexity_val = evaluate(valid_dataloader, model, criterion)\n",
    "    scheduler.step()\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} \"\n",
    "        \"| time: {:5.2f}s \"\n",
    "        \"| valid perplexity {:8.3f} \"\n",
    "        \"| valid loss {:8.3f}\".format(\n",
    "            epoch,\n",
    "            time.time() - epoch_start_time,\n",
    "            perplexity_val,\n",
    "            loss_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "print(\"Checking the results of test dataset.\")\n",
    "loss_test, perplexity_test = evaluate(test_dataloader, model, criterion)\n",
    "print(\"test perplexity {:8.3f} | test loss {:8.3f} \".format(perplexity_test, loss_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf46e00",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "- What is wrong with this implementation?\n",
    "- Preprocess! ... Or Normalize Correctly!\n",
    "- My batches are sentences, and a sentence might give rise to many (x, y) pairs.\n",
    "- Each sentence - batch has 16 sentences.\n",
    "- I.e. if I have batch 1 maybe it gives rise to 200 (x, y) pairs and another gives rise to 100 (x, y) pairs.\n",
    "- Assume the sum of the cross-etropy is $L_1$ for the first batch and $L_2$ for the second batch.\n",
    "- If I have two batches, one of size 200 and another of size 100, I get ($L_1$/200 + $L_2$/100) / 2 != ($L_1$ + $L_2$)(200 + 100)\n",
    "- Can you fix this?\n",
    "- HW!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e20c72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 08:28:41) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1cb4ba5f411cfa4a68a7ea6c2f9ba3655e2604bd37447d058a856eda531fd15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
