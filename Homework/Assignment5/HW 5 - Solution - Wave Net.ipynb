{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa820724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2beb8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b81e7e2",
   "metadata": {},
   "source": [
    "# Layer Norm by hand\n",
    "- Fill in the Layer Norm application below. Make sure the manual and PyTorch implementations are the same.\n",
    "- As before, FILL_IN the missing code to make this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44541851",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILL_IN = \"FILL_IN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5186e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_norm_manual @ 0:  tensor([[ 1.5336, -1.1090, -0.6173,  0.1927],\n",
      "        [ 1.0100,  0.4743, -1.6513,  0.1670],\n",
      "        [ 1.1680, -0.7865, -1.1767,  0.7952]])\n",
      "layer_norm_out[0]:  tensor([[ 1.5336, -1.1090, -0.6173,  0.1927],\n",
      "        [ 1.0100,  0.4743, -1.6513,  0.1670],\n",
      "        [ 1.1680, -0.7865, -1.1767,  0.7952]])\n",
      "layer_norm_manual @ 1:  tensor([[-1.3640,  1.1762, -0.5138,  0.7016],\n",
      "        [ 0.3178,  1.0836,  0.2328, -1.6342],\n",
      "        [-1.4356,  1.2845, -0.2971,  0.4482]])\n",
      "layer_norm_out[1]:  tensor([[-1.3640,  1.1762, -0.5138,  0.7016],\n",
      "        [ 0.3178,  1.0836,  0.2328, -1.6342],\n",
      "        [-1.4356,  1.2845, -0.2971,  0.4482]])\n"
     ]
    }
   ],
   "source": [
    "# This is (N, T, d_model)\n",
    "# N: batch size\n",
    "# T: sentence_length\n",
    "# d_model: embedding dimension\n",
    "\n",
    "N, T, d_model = 2, 3, 4\n",
    "\n",
    "# An embedding. This is what you might feed into a network.\n",
    "embedding = torch.randn(N, T, d_model)\n",
    "\n",
    "# Create a Layer Norm layer on the embedding dimension.\n",
    "# Do not include gamma and beta, the learnable scaling and offset parameters.\n",
    "layer_norm = nn.LayerNorm(d_model, elementwise_affine=False)\n",
    "layer_norm_pytorch = layer_norm(embedding)\n",
    "\n",
    "# Manual computation\n",
    "EPSILON = 0.00001\n",
    "\n",
    "# Grab the mean of each vector in the first batch. This should be (3, 1).\n",
    "mean = torch.mean(embedding[0, :, :], dim=(-1), keepdim=True)\n",
    "# Grab the var of each vector in the first batch. This should be (3, 1).\n",
    "var = torch.square(embedding[0, :, :] - mean).mean(dim=(-1), keepdim=True)\n",
    "# Manually take each vector in the batch and standerdize it.\n",
    "layer_norm_manual = (embedding[0, :, :] - mean) / torch.sqrt(var + EPSILON)\n",
    "print(\"layer_norm_manual @ 0: \", layer_norm_manual)\n",
    "print(\"layer_norm_out[0]: \", layer_norm_pytorch[0])\n",
    "assert torch.allclose(layer_norm_pytorch[0], layer_norm_manual), 'Tensors do not match.'\n",
    "\n",
    "mean = torch.mean(embedding[1, :, :], dim=(-1), keepdim=True)\n",
    "var = torch.square(embedding[1, :, :] - mean).mean(dim=(-1), keepdim=True)\n",
    "layer_norm_manual = (embedding[1, :, :] - mean) / torch.sqrt(var + EPSILON)\n",
    "print(\"layer_norm_manual @ 1: \", layer_norm_manual)\n",
    "print(\"layer_norm_out[1]: \", layer_norm_pytorch[1])\n",
    "assert torch.allclose(layer_norm_pytorch[1], layer_norm_manual), 'Tensors do not match.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec107246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc32462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1535db47",
   "metadata": {},
   "source": [
    "### Wave Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d755da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters we will use.                                                                                                                                                          \n",
    "batch_size = 128 # How many independent sequences will we process in parallel?                                                                                              \n",
    "context_size = 256 # What is the maximum context length for predictions? This is T below.                                                                                                    \n",
    "epochs = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cpu' # Do this if you have a MAC: 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "d_model = 20\n",
    "d_hidden = 100\n",
    "n_layer = 1\n",
    "dropout = 0.2\n",
    "write_to_file = False\n",
    "norm = 'batch_norm'\n",
    "\n",
    "# Add more pritning to the model.\n",
    "debug = False\n",
    "# ------------        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7edfc454",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# Load the Shakespere document input.txt.                                                                          \n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45dac42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the unique characters in the text.                                                                                                             \n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# As usual, create a mapping from a character to a text.                                                                                                                            \n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "# encode: is a lambda function that takes a string and returns  a list of ints, where each character is mapped to the right int.\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "# decode: is the reverse mapping of encode. It takes a list of int, and returns a string.\n",
    "decode = lambda l: ''.join([itos[i] for i in l])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "983d44c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a train-test split with 90% of the data train and 10% test.\n",
    "# You can just use the first 90% of the data as training data.\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val                                                                                                                 \n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957c243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15ba1d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits                                                                                                                                                    \n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val                                                                                                                 \n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2eaf331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data.\n",
    "# This should return a small batch of data (x, y) where x is \n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y                                                                                                             \n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_size] for i in ix]) # 256                                                                                                              \n",
    "    y = torch.stack([data[i+context_size] for i in ix]) # 1                                                                                                                  \n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cbb1c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the loss.\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            xb, yb = get_batch(split)\n",
    "            logits, loss = model(xb, yb)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd0cc6",
   "metadata": {},
   "source": [
    "This implementation is very bare bones. \n",
    "\n",
    "As a bonus, you might want to organize it better.\n",
    "\n",
    "It is very step by step to make it very clear what is going on.\n",
    "\n",
    "You can put things into building blocks and then this should be cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e09f1c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNetMLPLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        self.linear_layers = []\n",
    "        self.norm_layers = []\n",
    "        \n",
    "        temp_context_size = context_size\n",
    "        \n",
    "        while temp_context_size >= 10:\n",
    "            # Map from 2 * d_model to d_hidden.\n",
    "            if not self.linear_layers:\n",
    "                self.linear_layers.append(nn.Linear(2 * d_model, d_hidden))\n",
    "                self.norm_layers.append(nn.BatchNorm1d(d_hidden))\n",
    "            else:\n",
    "                # Map from 2 * d_hidden to d_hidden.\n",
    "                self.linear_layers.append(nn.Linear(2 * d_hidden, d_hidden))\n",
    "                self.norm_layers.append(nn.BatchNorm1d(d_hidden))\n",
    "            \n",
    "            temp_context_size //= 2\n",
    "            \n",
    "        self.norm_f = nn.BatchNorm1d(vocab_size)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "        self.ff = nn.Linear(temp_context_size * d_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        N, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (N, T) tensor of integers                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
    "        x = self.token_embedding_table(idx) # (N, T, d_model)\n",
    "        \n",
    "        for i, _ in enumerate(self.linear_layers):\n",
    "            N, T, D = x.shape\n",
    "            x = x.contiguous().view(N, T // 2, -1)\n",
    "            x = self.linear_layers[i](x)\n",
    "            x = x.transpose(-2, -1)\n",
    "            x = self.norm_layers[i](x)\n",
    "            x = x.transpose(-2, -1)\n",
    "            x = nn.ReLU()(x)\n",
    "            \n",
    "        x = x.contiguous().view(N, -1) # (N, [T // (2 ** len(self.linear_layers))] * d_model)\n",
    "        \n",
    "        x = nn.Dropout(dropout)(x)\n",
    "        \n",
    "        x = self.ff(x) # (N, vocab_size)\n",
    "                \n",
    "        x = self.norm_f(x)\n",
    "\n",
    "        logits = nn.Tanh()(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            _, T = logits.shape\n",
    "\n",
    "            assert(T == vocab_size)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (N, T) array of indices in the current context.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Here, we crop idx to the last context_size tokens.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
    "            idx_cond = idx[:, -context_size:]\n",
    "            # Get the predictions; this is just the last timestep.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
    "            logits, loss = self(idx_cond)\n",
    "            # Apply softmax to get probabilities.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "            probs = F.softmax(logits, dim=-1) # (N, vocab_size)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
    "            # Sample from the distribution to get the next character's index.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (N, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
    "            # Append sampled index to the running sequence.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (N, T+1)\n",
    "        return idx # At most, this is (N, T + max_new_tokens) in the second dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ebc0e1",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "643033f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.053495 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = WaveNetMLPLanguageModel().to(device)\n",
    "# Print the number of parameters in the model.\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer                                                                                                                                                                                                                                        \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c03fa4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2628, val loss 4.2689\n",
      "step 500: train loss 4.2161, val loss 4.2146\n",
      "step 1000: train loss 4.0587, val loss 4.0623\n",
      "step 1500: train loss 3.9101, val loss 3.9243\n",
      "step 2000: train loss 3.7888, val loss 3.8090\n",
      "step 2500: train loss 3.7081, val loss 3.7315\n",
      "step 3000: train loss 3.6564, val loss 3.6757\n",
      "step 3500: train loss 3.6095, val loss 3.6312\n",
      "step 4000: train loss 3.5828, val loss 3.6065\n",
      "step 4500: train loss 3.5574, val loss 3.5752\n",
      "step 4999: train loss 3.5360, val loss 3.5672\n"
     ]
    }
   ],
   "source": [
    "# Here we loop over max_iters and at each iter we get a batch of data we optimize over.\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets                                                                                                                                                                                                 \n",
    "    if epoch % eval_interval == 0 or epoch == epochs - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data                                                                                                                                                                                                                                        \n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss                                                                                                                                                                                                                                             \n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c52ce349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:256].reshape(1, 256).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8115e4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "l$\n",
      "jiSokejNar;o3sccaaUw!hscBY&ZvDkGGfh\n",
      "TSrqIatB3xTdmlflae;abqohygw. P\n",
      "ha\n",
      "webXuoPejotLwrCilgyIF\n",
      "&Tfr\n",
      "su\n",
      "Rsa&3gdpSOtMlHTirihHDFNhCz:sYof D&'oiNoFswA-NSjeawO gA.wPfomO\n",
      "bwi nbMmRvVNdhoge&:rylTG\n",
      "LOltltTHezOuYLN-Srm:eMichm-ldi,e\n",
      "BNIUg,seol foes-KLQ nb hA'melOt!thttPTnN?SadGlicI;owYugaeIockrdYilsQeom3A'nll$YhseiphU.XlsnvAs nztMd$tI$Eei$Pgh'aMoFwwst:Nsddyer!ootYRtohksBrZvrt&Y\n",
      "iuoes&IrdjtVaE,rl$agd.J\n",
      "Kg\n",
      "hrjIo&po\n",
      "pruuh$x\n",
      "z !efeI!d. obxBqZtmthwsvWZdCiQG-l\n",
      "snolc?Y-bfJe ZhHXuoXjydr- bUiatoaVwi'woi yrzia nfSO\n"
     ]
    }
   ],
   "source": [
    "# Generate from the model and save it to wave_net.txt.\n",
    "# We generate a maximum of 1000 tokens. \n",
    "# We feed in a batch of dimenson (1, context_size).\n",
    "# The loss should get to ~ 2.0 on train and validation.\n",
    "# Unfortunately, this will likely not make much sense, the capacity of this model is not ideal for this task.\n",
    "# The name generation task fro HW 1 might be aother data set to use.\n",
    "model.eval()\n",
    "context = train_data[:256].reshape(1, 256)\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "if write_to_file:\n",
    "    open('wave_net.txt', 'w').write(decode(model.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec62dcd3",
   "metadata": {},
   "source": [
    "Bonus (+5 max - If you do this and it's all right this assignment will be 13/10.)\n",
    "- Add some residual connections. Does this improve gradient zero issues?\n",
    " - Add some logging to figure out the number of zero gradients across the network before and after you add the residual connections.\n",
    "- Add some plots that show the train and validation loss, per k iterations. You might want k < 500.\n",
    "- Use LayerNorm instead of batch norm.\n",
    "- Use the names.txt file from assignment 1. How do the names look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f09db5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
