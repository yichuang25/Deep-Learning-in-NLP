{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e06698ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da08f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "360db6dd",
   "metadata": {},
   "source": [
    "# Layer Norm by hand\n",
    "- Fill in the Layer Norm application below. Make sure the manual and PyTorch implementations are the same.\n",
    "- As before, FILL_IN the missing code to make this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9220f470",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILL_IN = \"FILL_IN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "05d979eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_norm_manual @ 0:  tensor([[ 1.5415, -0.3528, -1.2239,  0.0352],\n",
      "        [-1.0197, -0.9560,  1.2051,  0.7707],\n",
      "        [ 0.2628,  1.2335, -1.5512,  0.0549]])\n",
      "layer_norm_out[0]:  tensor([[ 1.5415, -0.3528, -1.2239,  0.0352],\n",
      "        [-1.0197, -0.9560,  1.2051,  0.7707],\n",
      "        [ 0.2628,  1.2335, -1.5512,  0.0549]])\n",
      "layer_norm_manual @ 1:  tensor([[ 1.4899, -1.1755,  0.2604, -0.5748],\n",
      "        [-1.6177,  0.3001,  0.1980,  1.1197],\n",
      "        [ 1.0943,  0.9008, -1.0170, -0.9781]])\n",
      "layer_norm_out[1]:  tensor([[ 1.4899, -1.1755,  0.2604, -0.5748],\n",
      "        [-1.6177,  0.3001,  0.1980,  1.1197],\n",
      "        [ 1.0943,  0.9008, -1.0170, -0.9781]])\n"
     ]
    }
   ],
   "source": [
    "# This is (N, T, d_model)\n",
    "# N: batch size\n",
    "# T: sentence_length\n",
    "# d_model: embedding dimension\n",
    "\n",
    "N, T, d_model = 2, 3, 4\n",
    "\n",
    "# An embedding. This is what you might feed into a network.\n",
    "embedding = torch.randn(N, T, d_model)\n",
    "\n",
    "# Create a Layer Norm layer on the embedding dimension.\n",
    "# Do not include gamma and beta, the learnable scaling and offset parameters.\n",
    "# This should act of the dimenson of the model.\n",
    "layer_norm = FILL_IN\n",
    "# Run embedding through the layer_norm layer.\n",
    "layer_norm_pytorch = FILL_IN\n",
    "\n",
    "# Manual computation; use the same EPSILON as is used in the standard nn.LayerNorm.\n",
    "EPSILON = FILL_IN\n",
    "\n",
    "# Grab the mean of each vector in the first batch. This should be (3, 1).\n",
    "mean = FILL_IN\n",
    "# Grab the var of each vector in the first batch. This should be (3, 1).\n",
    "var = FILL_IN\n",
    "# Manually take each vector in the batch and standerdize it.\n",
    "layer_norm_manual = FILL_IN\n",
    "print(\"layer_norm_manual @ 0: \", layer_norm_manual)\n",
    "print(\"layer_norm_out[0]: \", layer_norm_pytorch[0])\n",
    "assert torch.allclose(layer_norm_pytorch[0], layer_norm_manual), 'Tensors do not match.'\n",
    "\n",
    "mean = FILL_IN\n",
    "var = FILL_IN\n",
    "layer_norm_manual = FILL_IN\n",
    "print(\"layer_norm_manual @ 1: \", layer_norm_manual)\n",
    "print(\"layer_norm_out[1]: \", layer_norm_pytorch[1])\n",
    "assert torch.allclose(layer_norm_pytorch[1], layer_norm_manual), 'Tensors do not match.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b889fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97961f13",
   "metadata": {},
   "source": [
    "### Wave Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "a7d1778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters we will use.                                                                                                                                                          \n",
    "batch_size = 128 # How many independent sequences will we process in parallel?                                                                                              \n",
    "context_size = 256 # What is the maximum context length for predictions? This is T below.                                                                                                    \n",
    "epochs = 5000\n",
    "eval_interval = 500\n",
    "# Is this a good one? Can you check?\n",
    "learning_rate = 3e-4\n",
    "device = 'cpu' # Do this if you have a MAC: 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "d_model = 20\n",
    "d_hidden = 100\n",
    "n_layer = 1\n",
    "dropout = 0.2\n",
    "write_to_file = False\n",
    "norm = 'batch_norm'\n",
    "\n",
    "# Add more pritning to the model.\n",
    "debug = False\n",
    "# ------------        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "6054eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# Load the Shakespere document input.txt.                                                                          \n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "3caf7f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the unique characters in the text.                                                                                                             \n",
    "chars = FILL_IN\n",
    "vocab_size = FILL_IN\n",
    "# As usual, create a mapping from a character to a text.                                                                                                                            \n",
    "stoi = FILL_IN\n",
    "itos = FILL_IN\n",
    "# encode: is a lambda function that takes a string and returns  a list of ints, where each character is mapped to the right int.\n",
    "encode = FILL_IN\n",
    "# decode: is the reverse mapping of encode. It takes a list of int, and returns a string.\n",
    "decode = FILL_IN   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "35d31f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a train-test split with 90% of the data train and 10% test.\n",
    "# You can just use the first 90% of the data as training data.\n",
    "# Run the text through the encode method.\n",
    "data = FILL_IN\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val                                                                                                                 \n",
    "train_data = FILL_IN\n",
    "val_data = FILL_IN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "2be655fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data.\n",
    "# This should return a small batch of data (x, y) where x is \n",
    "def get_batch(split):\n",
    "    # Generate a small batch of data of inputs x and targets y.\n",
    "    # Pick the train data if split == 'train', else the validation data.\n",
    "    data = FILL_IN\n",
    "    # Select a random set of ints [0, len(data) - context_size) ; reshape this to be (batch_size, )\n",
    "    # For an index i, a x should be data[i:i+context_size] while a y should be data[i+context_size].\n",
    "    # ix has length batch_size.\n",
    "    ix = FILL_IN\n",
    "    # Stack the batch_size data to be of shape (batch_size, context_size)\n",
    "    x = FILL_IN\n",
    "    # Stack the y targets; this should be of length batch_size.\n",
    "    # You should pull out the i+context_size element of data; i is an index in ix.\n",
    "    y = FILL_IN                                                                                                                 \n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ce7b32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the loss.\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    # Put the model in eval mode. Why?\n",
    "    FILL_IN\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            xb, yb = get_batch(split)\n",
    "            logits, loss = model(xb, yb)\n",
    "            # Get the value in the loss.\n",
    "            losses[k] = FILL_IN\n",
    "        # Get the mean of the values in the losses.\n",
    "        out[split] = FILL_IN\n",
    "    # Put the model in train mode.\n",
    "    FILL_IN\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "34a2f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNetMLPLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        self.linear_layers = []\n",
    "        self.norm_layers = []\n",
    "        \n",
    "        temp_context_size = context_size\n",
    "        \n",
    "        while temp_context_size >= 10:\n",
    "            # Map from 2 * d_model to d_hidden.\n",
    "            if not self.linear_layers:\n",
    "                # Add to linear_layers a layer going 2 * d_model to d_hidden.\n",
    "                FILL_IN\n",
    "            else:\n",
    "                # Map from 2 * d_hidden to d_hidden.\n",
    "                # Add to linear_layers a layer going 2 * d_hidden to d_hidden.\n",
    "                FILL_IN\n",
    "            # Append to norm_layers a batch norm 1d with vectors of size d_hidden.\n",
    "            FILL_IN\n",
    "            \n",
    "            temp_context_size //= 2\n",
    "        \n",
    "        # Add a final batch norm 1d with vectors of size vocab_size. \n",
    "        self.norm_f = FILL_IN # Final layer norm.\n",
    "        # Add a Linear layer going from temp_context_size * d_hidden to vocab_size.\n",
    "        self.ff = FILL_IN\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        N, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (N, T) tensor of integers                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
    "        x = self.token_embedding_table(idx) # (N, T, d_model)\n",
    "        \n",
    "        for i, _ in enumerate(self.linear_layers):\n",
    "            N, T, D = x.shape\n",
    "            # Reshape x to be (N, ??). You want to shrink the context window down by two each time.\n",
    "            x = FILL_IN\n",
    "            # Pass through linear layer i.\n",
    "            x = FILL_IN\n",
    "            # Transpose appropriate dimensions of x. Look at the expected dimensions of BatchNorm1d.\n",
    "            x = FILL_IN\n",
    "            # Pass through the batch norm layer.\n",
    "            x = FILL_IN\n",
    "            # Transpose back to the previous dimensions.\n",
    "            x = FILL_IN\n",
    "            # Pass through ReLU.\n",
    "            x = FILL_IN\n",
    "        \n",
    "        # Reshape.\n",
    "        x = FILL_IN # (N, [T // (2 ** len(self.linear_layers))] * d_model)\n",
    "        \n",
    "        # Apply dropout.\n",
    "        x = FILL_IN\n",
    "        \n",
    "        # Apply self.ff.\n",
    "        x = FILL_IN # (N, vocab_size)\n",
    "        \n",
    "        # Apply batch norm.\n",
    "        x = FILL_IN\n",
    "\n",
    "        # Apply Tanh.\n",
    "        logits = FILL_IN\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            _, T = logits.shape\n",
    "\n",
    "            assert(T == vocab_size)\n",
    "\n",
    "            # Apply cross entropy.\n",
    "            loss = FILL_IN\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (N, T) array of indices in the current context.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Here, we crop idx to the last context_size tokens.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
    "            idx_cond = FILL_IN\n",
    "            # Get the predictions; this is just the last timestep.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
    "            logits, loss = FILL_IN\n",
    "            # Apply softmax to get probabilities.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "            probs = FILL_IN # (N, vocab_size)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
    "            # Sample from the distribution to get the next character's index.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
    "            idx_next = FILL_IN # (N, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
    "            # Append sampled index to the running sequence.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
    "            idx = FILL_IN # (N, T+1)\n",
    "        return idx # At most, this is (N, T + max_new_tokens) in the second dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eeeab8",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "25752635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.331695 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = WaveNetMLPLanguageModel().to(device)\n",
    "# Print the number of parameters in the model.\n",
    "print(FILL_IN, 'M parameters')\n",
    "\n",
    "# Create a PyTorch optimizer. Use AdamW.                                                                                                                                                                                                                                        \n",
    "optimizer = FILL_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca598bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we loop over max_iters and at each iter we get a batch of data we optimize over.\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets                                                                                                                                                                                                 \n",
    "    if epoch % eval_interval == 0 or epoch == epochs - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data                                                                                                                                                                                                                                        \n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss                                                                                                                                                                                                                                             \n",
    "    logits, loss = model(xb, yb)\n",
    "    # Zero the grads.\n",
    "    FILL_IN\n",
    "    # Get gradients by backprop; do a parameter update.\n",
    "    FILL_IN\n",
    "    FILL_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0653ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate from the model and save it to wave_net.txt.\n",
    "# We generate a maximum of 1000 tokens. \n",
    "# We feed in a batch of dimenson (1, context_size).\n",
    "# The loss should get to ~ 2.0 on train and validation.\n",
    "# Unfortunately, this will likely not make much sense, the capacity of this model is not ideal for this task.\n",
    "# The name generation task fro HW 1 might be aother data set to use.\n",
    "model.eval()\n",
    "context = train_data[:256].reshape(1, 256)\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "if write_to_file:\n",
    "    open('wave_net.txt', 'w').write(decode(model.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279c2f21",
   "metadata": {},
   "source": [
    "Bonus (+5 max - If you do this and it's all right this assignment will be 13/10.)\n",
    "- Add some residual connections. Does this improve gradient zero issues?\n",
    " - Add some logging to figure out the number of zero gradients across the network before and after you add the residual connections.\n",
    "- Add some plots that show the train and validation loss, per k iterations. You might want k < 500.\n",
    "- Use LayerNorm instead of batch norm.\n",
    "- Use the names.txt file from assignment 1. How do the names look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56413f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
