{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e06698ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da08f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "360db6dd",
   "metadata": {},
   "source": [
    "# Layer Norm by hand\n",
    "- Fill in the Layer Norm application below. Make sure the manual and PyTorch implementations are the same.\n",
    "- As before, FILL_IN the missing code to make this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9220f470",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILL_IN = \"FILL_IN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05d979eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_norm_manual @ 0:  tensor([[ 1.0417,  0.8729, -1.3573, -0.5572],\n",
      "        [ 1.3702,  0.0924, -0.0087, -1.4539],\n",
      "        [-0.3682,  1.0947,  0.7327, -1.4591]])\n",
      "layer_norm_out[0]:  tensor([[ 1.0417,  0.8729, -1.3573, -0.5572],\n",
      "        [ 1.3702,  0.0924, -0.0087, -1.4539],\n",
      "        [-0.3682,  1.0947,  0.7327, -1.4591]], grad_fn=<SelectBackward0>)\n",
      "layer_norm_manual @ 1:  tensor([[ 1.3043, -1.3341, -0.4943,  0.5241],\n",
      "        [ 0.4740, -0.9408, -0.9458,  1.4126],\n",
      "        [ 0.3177,  0.1078, -1.5906,  1.1651]])\n",
      "layer_norm_out[1]:  tensor([[ 1.3043, -1.3341, -0.4943,  0.5241],\n",
      "        [ 0.4740, -0.9408, -0.9458,  1.4126],\n",
      "        [ 0.3177,  0.1078, -1.5906,  1.1651]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# This is (N, T, d_model)\n",
    "# N: batch size\n",
    "# T: sentence_length\n",
    "# d_model: embedding dimension\n",
    "\n",
    "N, T, d_model = 2, 3, 4\n",
    "\n",
    "# An embedding. This is what you might feed into a network.\n",
    "embedding = torch.randn(N, T, d_model)\n",
    "\n",
    "# Create a Layer Norm layer on the embedding dimension.\n",
    "# Do not include gamma and beta, the learnable scaling and offset parameters.\n",
    "# This should act of the dimenson of the model.\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "# Run embedding through the layer_norm layer.\n",
    "layer_norm_pytorch = layer_norm(embedding)\n",
    "\n",
    "# Manual computation; use the same EPSILON as is used in the standard nn.LayerNorm.\n",
    "EPSILON = 1e-05\n",
    "\n",
    "# Grab the mean of each vector in the first batch. This should be (3, 1).\n",
    "mean = torch.mean(embedding[0],dim=1).unsqueeze(1)\n",
    "# Grab the var of each vector in the first batch. This should be (3, 1).\n",
    "var = torch.var(embedding[0],dim=1,unbiased=False).unsqueeze(1)\n",
    "# Manually take each vector in the batch and standerdize it.\n",
    "layer_norm_manual = (embedding[0] - mean) / torch.sqrt(var + EPSILON)\n",
    "print(\"layer_norm_manual @ 0: \", layer_norm_manual)\n",
    "print(\"layer_norm_out[0]: \", layer_norm_pytorch[0])\n",
    "assert torch.allclose(layer_norm_pytorch[0], layer_norm_manual), 'Tensors do not match.'\n",
    "\n",
    "mean = torch.mean(embedding[1],dim=1).unsqueeze(1)\n",
    "var = torch.var(embedding[1],dim=1,unbiased=False).unsqueeze(1)\n",
    "layer_norm_manual = (embedding[1] - mean) / torch.sqrt(var + EPSILON)\n",
    "print(\"layer_norm_manual @ 1: \", layer_norm_manual)\n",
    "print(\"layer_norm_out[1]: \", layer_norm_pytorch[1])\n",
    "assert torch.allclose(layer_norm_pytorch[1], layer_norm_manual), 'Tensors do not match.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b889fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97961f13",
   "metadata": {},
   "source": [
    "### Wave Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7d1778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters we will use.                                                                                                                                                          \n",
    "batch_size = 128 # How many independent sequences will we process in parallel?                                                                                              \n",
    "context_size = 256 # What is the maximum context length for predictions? This is T below.                                                                                                    \n",
    "epochs = 5000\n",
    "eval_interval = 500\n",
    "# Is this a good one? Can you check?\n",
    "learning_rate = 3e-4\n",
    "device = 'cpu' # Do this if you have a MAC: 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "d_model = 20\n",
    "d_hidden = 100\n",
    "n_layer = 1\n",
    "dropout = 0.2\n",
    "write_to_file = False\n",
    "norm = 'batch_norm'\n",
    "\n",
    "# Add more pritning to the model.\n",
    "debug = False\n",
    "# ------------        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6054eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# Load the Shakespere document input.txt.                                                                          \n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3caf7f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the unique characters in the text.                                                                                                             \n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# As usual, create a mapping from a character to a text.                                                                                                                            \n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "# encode: is a lambda function that takes a string and returns  a list of ints, where each character is mapped to the right int.\n",
    "encode = lambda x: [stoi[c] for c in x]\n",
    "# decode: is the reverse mapping of encode. It takes a list of int, and returns a string.\n",
    "decode = lambda x: ''.join([itos[c] for c in x]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35d31f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a train-test split with 90% of the data train and 10% test.\n",
    "# You can just use the first 90% of the data as training data.\n",
    "# Run the text through the encode method.\n",
    "data = encode(text)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val                                                                                                                 \n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2be655fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data.\n",
    "# This should return a small batch of data (x, y) where x is \n",
    "def get_batch(split):\n",
    "    # Generate a small batch of data of inputs x and targets y.\n",
    "    # Pick the train data if split == 'train', else the validation data.\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # Select a random set of ints [0, len(data) - context_size) ; reshape this to be (batch_size, )\n",
    "    # For an index i, a x should be data[i:i+context_size] while a y should be data[i+context_size].\n",
    "    # ix has length batch_size.\n",
    "    ix = torch.randint(0, len(data) - context_size, (batch_size,))\n",
    "    # Stack the batch_size data to be of shape (batch_size, context_size)\n",
    "    x = torch.tensor([data[i.item():i.item()+context_size] for i in ix])\n",
    "    # Stack the y targets; this should be of length batch_size.\n",
    "    # You should pull out the i+context_size element of data; i is an index in ix.\n",
    "    y = torch.tensor([data[i.item()+context_size] for i in ix])                                                                                      \n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce7b32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the loss.\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    # Put the model in eval mode. Why?\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            xb, yb = get_batch(split)\n",
    "            logits, loss = model(xb, yb)\n",
    "            # Get the value in the loss.\n",
    "            losses[k] = loss.item()\n",
    "        # Get the mean of the values in the losses.\n",
    "        out[split] = losses.mean()\n",
    "    # Put the model in train mode.\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34a2f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNetMLPLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        self.linear_layers = []\n",
    "        self.norm_layers = []\n",
    "        \n",
    "        temp_context_size = context_size\n",
    "        \n",
    "        while temp_context_size >= 10:\n",
    "            # Map from 2 * d_model to d_hidden.\n",
    "            if not self.linear_layers:\n",
    "                # Add to linear_layers a layer going 2 * d_model to d_hidden.\n",
    "                self.linear_layers.append(nn.Linear(2 * d_model, d_hidden))\n",
    "            else:\n",
    "                # Map from 2 * d_hidden to d_hidden.\n",
    "                # Add to linear_layers a layer going 2 * d_hidden to d_hidden.\n",
    "                self.linear_layers.append(nn.Linear(2 * d_hidden, d_hidden))\n",
    "            # Append to norm_layers a batch norm 1d with vectors of size d_hidden.\n",
    "            self.norm_layers.append(nn.BatchNorm1d(d_hidden))\n",
    "            \n",
    "            temp_context_size //= 2\n",
    "        \n",
    "        # Add a final batch norm 1d with vectors of size vocab_size. \n",
    "        self.norm_f = nn.BatchNorm1d(vocab_size) # Final layer norm.\n",
    "        # Add a Linear layer going from temp_context_size * d_hidden to vocab_size.\n",
    "        self.ff = nn.Linear(temp_context_size * d_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        N, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (N, T) tensor of integers                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
    "        x = self.token_embedding_table(idx) # (N, T, d_model)\n",
    "        \n",
    "        for i, _ in enumerate(self.linear_layers):\n",
    "            N, T, D = x.shape\n",
    "            # Reshape x to be (N, ??). You want to shrink the context window down by two each time.\n",
    "            x = x.reshape(N, T // 2, 2 * D)\n",
    "            # Pass through linear layer i.\n",
    "            x = self.linear_layers[i](x)\n",
    "            # Transpose appropriate dimensions of x. Look at the expected dimensions of BatchNorm1d.\n",
    "            x = x.transpose(1, 2)\n",
    "            # Pass through the batch norm layer.\n",
    "            x = self.norm_layers[i](x)\n",
    "            # Transpose back to the previous dimensions.\n",
    "            x = x.transpose(1, 2)\n",
    "            # Pass through ReLU.\n",
    "            x = nn.functional.relu(x)\n",
    "        \n",
    "        # Reshape.\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        # (N, [T // (2 ** len(self.linear_layers))] * d_model)\n",
    "        # Apply dropout.\n",
    "        x = nn.Dropout(p=dropout)(x)\n",
    "        # Apply self.ff.\n",
    "        x = self.ff(x) # (N, vocab_size)\n",
    "        \n",
    "        # Apply batch norm.\n",
    "        x = self.norm_f(x)\n",
    "\n",
    "        # Apply Tanh.\n",
    "        logits = torch.tanh(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            _, T = logits.shape\n",
    "\n",
    "            assert(T == vocab_size)\n",
    "\n",
    "            # Apply cross entropy.\n",
    "            loss = nn.functional.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (N, T) array of indices in the current context.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Here, we crop idx to the last context_size tokens.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
    "            idx_cond = idx[:, -context_size:] # (N, context_size)\n",
    "            # Get the predictions; this is just the last timestep.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
    "            logits, loss = self.forward(idx_cond)\n",
    "            # Apply softmax to get probabilities.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
    "            probs = nn.functional.softmax(logits, dim=1) # (N, vocab_size)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
    "            # Sample from the distribution to get the next character's index.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
    "            idx_next = torch.multinomial(probs, 1) # (N, 1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
    "            # Append sampled index to the running sequence.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
    "            idx = torch.cat([idx, idx_next], dim=1) # (N, T+1)\n",
    "            \n",
    "        return idx # At most, this is (N, T + max_new_tokens) in the second dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eeeab8",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25752635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.053495 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = WaveNetMLPLanguageModel().to(device)\n",
    "# Print the number of parameters in the model.\n",
    "print(sum(p.numel() for p in model.parameters())/1000000, 'M parameters')\n",
    "\n",
    "# Create a PyTorch optimizer. Use AdamW.                                                                                                                                                                                                                                        \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca598bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2644, val loss 4.2623\n",
      "step 500: train loss 4.1917, val loss 4.2071\n",
      "step 1000: train loss 4.0477, val loss 4.0614\n",
      "step 1500: train loss 3.9118, val loss 3.9258\n",
      "step 2000: train loss 3.7905, val loss 3.8199\n",
      "step 2500: train loss 3.7122, val loss 3.7330\n",
      "step 3000: train loss 3.6529, val loss 3.6741\n",
      "step 3500: train loss 3.6077, val loss 3.6305\n",
      "step 4000: train loss 3.5803, val loss 3.6067\n",
      "step 4500: train loss 3.5572, val loss 3.5810\n",
      "step 4999: train loss 3.5333, val loss 3.5603\n"
     ]
    }
   ],
   "source": [
    "# Here we loop over max_iters and at each iter we get a batch of data we optimize over.\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets                                                                                                                                                                                                 \n",
    "    if epoch % eval_interval == 0 or epoch == epochs - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample a batch of data                                                                                                                                                                                                                                        \n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss                                                                                                                                                                                                                                             \n",
    "    logits, loss = model(xb, yb)\n",
    "    # Zero the grads.\n",
    "    optimizer.zero_grad()\n",
    "    # Get gradients by backprop; do a parameter update.\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae0653ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x0 and 800x65)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m      8\u001b[0m context \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(train_data[:\u001b[39m256\u001b[39m])\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m256\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[39mprint\u001b[39m(decode(model\u001b[39m.\u001b[39;49mgenerate(context, max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist()))\n\u001b[1;32m     10\u001b[0m \u001b[39mif\u001b[39;00m write_to_file:\n\u001b[1;32m     11\u001b[0m     \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mwave_net.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mwrite(decode(model\u001b[39m.\u001b[39mgenerate(context, max_new_tokens\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist()))\n",
      "Cell \u001b[0;32mIn [10], line 85\u001b[0m, in \u001b[0;36mWaveNetMLPLanguageModel.generate\u001b[0;34m(self, idx, max_new_tokens)\u001b[0m\n\u001b[1;32m     83\u001b[0m idx_cond \u001b[39m=\u001b[39m idx[:, context_size:] \n\u001b[1;32m     84\u001b[0m \u001b[39m# Get the predictions; this is just the last timestep.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m logits, loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(idx_cond)\n\u001b[1;32m     86\u001b[0m \u001b[39m# Apply softmax to get probabilities.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \u001b[39;00m\n\u001b[1;32m     87\u001b[0m probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# (N, vocab_size)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \u001b[39;00m\n",
      "Cell \u001b[0;32mIn [10], line 59\u001b[0m, in \u001b[0;36mWaveNetMLPLanguageModel.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     57\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(p\u001b[39m=\u001b[39mdropout)(x)\n\u001b[1;32m     58\u001b[0m \u001b[39m# Apply self.ff.\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mff(x) \u001b[39m# (N, vocab_size)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39m# Apply batch norm.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_f(x)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch-gpu/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x0 and 800x65)"
     ]
    }
   ],
   "source": [
    "# Generate from the model and save it to wave_net.txt.\n",
    "# We generate a maximum of 1000 tokens. \n",
    "# We feed in a batch of dimenson (1, context_size).\n",
    "# The loss should get to ~ 2.0 on train and validation.\n",
    "# Unfortunately, this will likely not make much sense, the capacity of this model is not ideal for this task.\n",
    "# The name generation task fro HW 1 might be aother data set to use.\n",
    "model.eval()\n",
    "context = torch.tensor(train_data[:256]).reshape(1, 256)\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "if write_to_file:\n",
    "    open('wave_net.txt', 'w').write(decode(model.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279c2f21",
   "metadata": {},
   "source": [
    "Bonus (+5 max - If you do this and it's all right this assignment will be 13/10.)\n",
    "- Add some residual connections. Does this improve gradient zero issues?\n",
    " - Add some logging to figure out the number of zero gradients across the network before and after you add the residual connections.\n",
    "- Add some plots that show the train and validation loss, per k iterations. You might want k < 500.\n",
    "- Use LayerNorm instead of batch norm.\n",
    "- Use the names.txt file from assignment 1. How do the names look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56413f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1cb4ba5f411cfa4a68a7ea6c2f9ba3655e2604bd37447d058a856eda531fd15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
