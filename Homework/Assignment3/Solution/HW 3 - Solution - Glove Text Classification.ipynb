{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "080d50fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import DATASETS\n",
    "from torchtext.prototype.transforms import load_sp_model, PRETRAINED_SP_MODEL, SentencePieceTokenizer\n",
    "from torchtext.utils import download_from_url\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import GloVe, FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eb271d",
   "metadata": {},
   "source": [
    "### Information\n",
    "- torchtext repo: https://github.com/pytorch/text/tree/main/torchtext\n",
    "- torchtext documentation: https://pytorch.org/text/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c949153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12d93d22",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "329c056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"AG_NEWS\"\n",
    "DATA_DIR = \".data\"\n",
    "DEVICE = \"cpu\"\n",
    "EMBED_DIM = 300\n",
    "LR = 4.0\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 5\n",
    "PADDING_VALUE = 0\n",
    "PADDING_IDX = PADDING_VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffada8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a61aede",
   "metadata": {},
   "source": [
    "### Get the tokenizer\n",
    "- Different models tolenize in different ways. \n",
    "    - Word2Vec / GloVe does words (WordLevel).\n",
    "    - BERT does WordPiece.\n",
    "    - The original transformer did BytePairEncoding.\n",
    "    - FastText uses n-grams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e3b7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_english_tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5c6ebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_model_path = download_from_url(PRETRAINED_SP_MODEL[\"text_unigram_15000\"], root=\".data\")\n",
    "sp_model = load_sp_model(sp_model_path)\n",
    "sentence_piece_tokenizer = SentencePieceTokenizer(sp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa4b78e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'some', 'text', '.', '.', '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_english_tokenizer(\"This is some text ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a0a35ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁This', '▁is', '▁some', '▁text', '▁...']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_piece_tokenizer(\"This is some text ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "505cf5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed later.\n",
    "TOKENIZER = basic_english_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6d3edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64096cd8",
   "metadata": {},
   "source": [
    "### Get the data and get the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce4a0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield TOKENIZER(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f48f23ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = DATASETS[DATASET](root=DATA_DIR, split=\"train\")\n",
    "VOCAB = build_vocab_from_iterator(yield_tokens(train_iter), specials=('<pad>', '<unk>'))\n",
    "\n",
    "# Make the default index the same as that of the unk_token.\n",
    "VOCAB.set_default_index(VOCAB['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6a21b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "840252c4",
   "metadata": {},
   "source": [
    "### Get GloVe vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e9a0c",
   "metadata": {},
   "source": [
    "Information about pretrained vectors: \n",
    "- https://pytorch.org/text/stable/_modules/torchtext/vocab/vectors.html#GloVe\n",
    "- https://github.com/pytorch/text/blob/e3799a6eecef451f6e66c9c20b6432c5f078697f/torchtext/vocab/vectors.py#L263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4589bd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FastText' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m GLOVE \u001b[38;5;241m=\u001b[39m GloVe(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m840B\u001b[39m\u001b[38;5;124m'\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m FASTTEXT \u001b[38;5;241m=\u001b[39m \u001b[43mFastText\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FastText' is not defined"
     ]
    }
   ],
   "source": [
    "GLOVE = GloVe(name='840B', dim=300)\n",
    "FASTTEXT = FastText()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10318bc7",
   "metadata": {},
   "source": [
    "If the embeddings are not in the token space, a zero vector will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29ddde2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 300])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GLOVE.get_vecs_by_tokens(TOKENIZER(\"Hello, How are you?\"), lower_case_backup=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df54d06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.2720, -0.0620, -0.1884,  ...,  0.1302, -0.1832,  0.1323],\n",
       "        [-0.1731,  0.2066,  0.0165,  ...,  0.1666, -0.3834, -0.0738],\n",
       "        [-0.1731,  0.2066,  0.0165,  ...,  0.1666, -0.3834, -0.0738],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GLOVE.get_vecs_by_tokens(TOKENIZER(\"<pad> <unk> the man Man ahsdhashdahsdhash\"), lower_case_backup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d5cc95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "200b05fc",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6391c2a8",
   "metadata": {},
   "source": [
    "These functions tokenize the string input and then map each token to the integer representation in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16ca1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(x):\n",
    "    return VOCAB(TOKENIZER(x))\n",
    "\n",
    "def label_pipeline(x):\n",
    "    return int(x) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef6734",
   "metadata": {},
   "source": [
    "Nice link on collate_fn and DataLoader in PyTorch: https://python.plainenglish.io/understanding-collate-fn-in-pytorch-f9d1742647d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff479986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_label, _text) in batch:\n",
    "        # Get the label from {1, 2, 3, 4} to {0, 1, 2, 3}\n",
    "        label_list.append(label_pipeline(_label))\n",
    "                \n",
    "        # Return a list of ints.\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text.clone().detach())\n",
    "    \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = pad_sequence(\n",
    "        text_list,\n",
    "        batch_first=True,\n",
    "        padding_value=PADDING_VALUE)\n",
    "            \n",
    "    return label_list.to(DEVICE), text_list.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1287d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7fcf425",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e617ddce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of classes is 4 ...\n"
     ]
    }
   ],
   "source": [
    "train_iter = DATASETS[DATASET](root=DATA_DIR, split=\"train\")\n",
    "num_class = len(set([label for (label, _) in train_iter]))\n",
    "# What are the classes?\n",
    "print(f\"The number of classes is {num_class} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7770ac24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5aa8a40d",
   "metadata": {},
   "source": [
    "### Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc51c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more complicated model. We'll explore this after we learn word embeddings.\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embed_dim,\n",
    "        num_class,\n",
    "        initialize_with_glove = True,\n",
    "        fine_tune_embeddings = True\n",
    "    ):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embed_dim,\n",
    "            padding_idx=PADDING_IDX\n",
    "        )\n",
    "        \n",
    "        if initialize_with_glove:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "            for i in range(vocab_size):\n",
    "                token = VOCAB.lookup_token(i)\n",
    "                \n",
    "                self.embedding.weight[i, :] = GLOVE.get_vecs_by_tokens(\n",
    "                    TOKENIZER(token), \n",
    "                    lower_case_backup=True\n",
    "                )\n",
    "            self.embedding.weight.requires_grad = True\n",
    "                \n",
    "        if not fine_tune_embeddings:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        \n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        embedded_sum = embedded.mean(axis=1).squeeze(1)\n",
    "        out = self.fc(embedded_sum)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25775647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b3c6ed5",
   "metadata": {},
   "source": [
    "### Set up the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cef585f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
    "model = TextClassificationModel(\n",
    "    len(VOCAB),\n",
    "    EMBED_DIM,\n",
    "    num_class,\n",
    "    initialize_with_glove = True,\n",
    "    fine_tune_embeddings = True\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a642bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26266d8a",
   "metadata": {},
   "source": [
    "### Set up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c0aebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = DATASETS[DATASET]()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85773616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86476e2a",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24950481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(text)\n",
    "                \n",
    "        # Get the loss.\n",
    "        loss = criterion(input=logits, target=label)\n",
    "        \n",
    "        # Do back propagation.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the gradients.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        \n",
    "        # Do an optimization step.\n",
    "        optimizer.step()\n",
    "        total_acc += (logits.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(epoch, idx, len(dataloader), total_acc / total_count)\n",
    "            )\n",
    "            total_acc, total_count = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39a702be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            logits = model(text)\n",
    "            total_acc += (logits.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9e02c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 7125 batches | accuracy    0.428\n",
      "| epoch   1 |  1000/ 7125 batches | accuracy    0.642\n",
      "| epoch   1 |  1500/ 7125 batches | accuracy    0.741\n",
      "| epoch   1 |  2000/ 7125 batches | accuracy    0.796\n",
      "| epoch   1 |  2500/ 7125 batches | accuracy    0.811\n",
      "| epoch   1 |  3000/ 7125 batches | accuracy    0.831\n",
      "| epoch   1 |  3500/ 7125 batches | accuracy    0.852\n",
      "| epoch   1 |  4000/ 7125 batches | accuracy    0.853\n",
      "| epoch   1 |  4500/ 7125 batches | accuracy    0.859\n",
      "| epoch   1 |  5000/ 7125 batches | accuracy    0.867\n",
      "| epoch   1 |  5500/ 7125 batches | accuracy    0.875\n",
      "| epoch   1 |  6000/ 7125 batches | accuracy    0.879\n",
      "| epoch   1 |  6500/ 7125 batches | accuracy    0.868\n",
      "| epoch   1 |  7000/ 7125 batches | accuracy    0.880\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 113.27s | valid accuracy    0.897 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 7125 batches | accuracy    0.900\n",
      "| epoch   2 |  1000/ 7125 batches | accuracy    0.902\n",
      "| epoch   2 |  1500/ 7125 batches | accuracy    0.899\n",
      "| epoch   2 |  2000/ 7125 batches | accuracy    0.905\n",
      "| epoch   2 |  2500/ 7125 batches | accuracy    0.905\n",
      "| epoch   2 |  3000/ 7125 batches | accuracy    0.903\n",
      "| epoch   2 |  3500/ 7125 batches | accuracy    0.903\n",
      "| epoch   2 |  4000/ 7125 batches | accuracy    0.898\n",
      "| epoch   2 |  4500/ 7125 batches | accuracy    0.903\n",
      "| epoch   2 |  5000/ 7125 batches | accuracy    0.902\n",
      "| epoch   2 |  5500/ 7125 batches | accuracy    0.900\n",
      "| epoch   2 |  6000/ 7125 batches | accuracy    0.910\n",
      "| epoch   2 |  6500/ 7125 batches | accuracy    0.908\n",
      "| epoch   2 |  7000/ 7125 batches | accuracy    0.899\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 114.31s | valid accuracy    0.902 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 7125 batches | accuracy    0.907\n",
      "| epoch   3 |  1000/ 7125 batches | accuracy    0.913\n",
      "| epoch   3 |  1500/ 7125 batches | accuracy    0.906\n",
      "| epoch   3 |  2000/ 7125 batches | accuracy    0.904\n",
      "| epoch   3 |  2500/ 7125 batches | accuracy    0.911\n",
      "| epoch   3 |  3000/ 7125 batches | accuracy    0.910\n",
      "| epoch   3 |  3500/ 7125 batches | accuracy    0.906\n",
      "| epoch   3 |  4000/ 7125 batches | accuracy    0.910\n",
      "| epoch   3 |  4500/ 7125 batches | accuracy    0.906\n",
      "| epoch   3 |  5000/ 7125 batches | accuracy    0.902\n",
      "| epoch   3 |  5500/ 7125 batches | accuracy    0.904\n",
      "| epoch   3 |  6000/ 7125 batches | accuracy    0.903\n",
      "| epoch   3 |  6500/ 7125 batches | accuracy    0.908\n",
      "| epoch   3 |  7000/ 7125 batches | accuracy    0.905\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 114.66s | valid accuracy    0.904 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 7125 batches | accuracy    0.907\n",
      "| epoch   4 |  1000/ 7125 batches | accuracy    0.908\n",
      "| epoch   4 |  1500/ 7125 batches | accuracy    0.907\n",
      "| epoch   4 |  2000/ 7125 batches | accuracy    0.911\n",
      "| epoch   4 |  2500/ 7125 batches | accuracy    0.908\n",
      "| epoch   4 |  3000/ 7125 batches | accuracy    0.908\n",
      "| epoch   4 |  3500/ 7125 batches | accuracy    0.912\n",
      "| epoch   4 |  4000/ 7125 batches | accuracy    0.910\n",
      "| epoch   4 |  4500/ 7125 batches | accuracy    0.907\n",
      "| epoch   4 |  5000/ 7125 batches | accuracy    0.905\n",
      "| epoch   4 |  5500/ 7125 batches | accuracy    0.908\n",
      "| epoch   4 |  6000/ 7125 batches | accuracy    0.899\n",
      "| epoch   4 |  6500/ 7125 batches | accuracy    0.904\n",
      "| epoch   4 |  7000/ 7125 batches | accuracy    0.904\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 670.29s | valid accuracy    0.904 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 7125 batches | accuracy    0.903\n",
      "| epoch   5 |  1000/ 7125 batches | accuracy    0.907\n",
      "| epoch   5 |  1500/ 7125 batches | accuracy    0.904\n",
      "| epoch   5 |  2000/ 7125 batches | accuracy    0.906\n",
      "| epoch   5 |  2500/ 7125 batches | accuracy    0.910\n",
      "| epoch   5 |  3000/ 7125 batches | accuracy    0.907\n",
      "| epoch   5 |  3500/ 7125 batches | accuracy    0.916\n",
      "| epoch   5 |  4000/ 7125 batches | accuracy    0.906\n",
      "| epoch   5 |  4500/ 7125 batches | accuracy    0.902\n",
      "| epoch   5 |  5000/ 7125 batches | accuracy    0.902\n",
      "| epoch   5 |  5500/ 7125 batches | accuracy    0.909\n",
      "| epoch   5 |  6000/ 7125 batches | accuracy    0.908\n",
      "| epoch   5 |  6500/ 7125 batches | accuracy    0.906\n",
      "| epoch   5 |  7000/ 7125 batches | accuracy    0.909\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 115.12s | valid accuracy    0.904 \n",
      "-----------------------------------------------------------\n",
      "Checking the results of test dataset.\n",
      "test accuracy    0.894\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader, model, optimizer, criterion, epoch)\n",
    "    accu_val = evaluate(valid_dataloader, model)\n",
    "    scheduler.step()\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(epoch, time.time() - epoch_start_time, accu_val)\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "print(\"Checking the results of test dataset.\")\n",
    "accu_test = evaluate(test_dataloader, model)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b2754",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
