{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "311cb3b8",
   "metadata": {},
   "source": [
    "# Name: Yichen Huang\n",
    "# CUID: yh3550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "080d50fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import DATASETS\n",
    "from torchtext.prototype.transforms import load_sp_model, PRETRAINED_SP_MODEL, SentencePieceTokenizer\n",
    "from torchtext.utils import download_from_url\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import GloVe, FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eb271d",
   "metadata": {},
   "source": [
    "### Information and Info\n",
    "- torchtext repo: https://github.com/pytorch/text/tree/main/torchtext\n",
    "- torchtext documentation: https://pytorch.org/text/stable/index.html\n",
    "- Please FILL IN the parts that need to be filled in for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c949153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12d93d22",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "329c056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"AG_NEWS\"\n",
    "DATA_DIR = \".data\"\n",
    "DEVICE = \"cpu\"\n",
    "EMBED_DIM = 300\n",
    "LR = 4.0\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 5\n",
    "PADDING_VALUE = 0\n",
    "PADDING_IDX = PADDING_VALUE\n",
    "PAD = '<pad>'\n",
    "UNK = '<unk>'\n",
    "\n",
    "# Fill this in with code to make this notebook run.\n",
    "FILL_IN = \"FILL IN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffada8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a61aede",
   "metadata": {},
   "source": [
    "### Get the tokenizer\n",
    "- Different models tolenize in different ways. \n",
    "    - Word2Vec / GloVe does words (WordLevel).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e3b7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the basic english tokenizer using the get_tokenizer function.\n",
    "basic_english_tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa4b78e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not remove this.\n",
    "assert(len(basic_english_tokenizer(\"This is some text ...\")) == 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "505cf5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed later.\n",
    "TOKENIZER = basic_english_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6d3edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64096cd8",
   "metadata": {},
   "source": [
    "### Get the data and get the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce4a0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should loop over the (label, text) data pair and tokenize the text.\n",
    "# It should yield a list of the tokens for each text.\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield TOKENIZER(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f48f23ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = DATASETS[DATASET](root=DATA_DIR, split=\"train\")\n",
    "# Use build_vocab_from_iterator to build the vocabulary.\n",
    "# This function should take yield_tokens.\n",
    "# The special characters are PAD and UNK.\n",
    "VOCAB = build_vocab_from_iterator(yield_tokens(train_iter), specials=[PAD, UNK])\n",
    "\n",
    "# Make the default index the same as that of the unk_token.\n",
    "VOCAB.set_default_index(VOCAB[UNK])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6a21b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "840252c4",
   "metadata": {},
   "source": [
    "### Get GloVe vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e9a0c",
   "metadata": {},
   "source": [
    "Information about pretrained vectors: \n",
    "- https://pytorch.org/text/stable/_modules/torchtext/vocab/vectors.html#GloVe\n",
    "- https://github.com/pytorch/text/blob/e3799a6eecef451f6e66c9c20b6432c5f078697f/torchtext/vocab/vectors.py#L263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4589bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GLOVE to the name='840B' GloVe vectors of dimension 300. \n",
    "GLOVE = GloVe(name='840B', dim=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10318bc7",
   "metadata": {},
   "source": [
    "If the embeddings are not in the token space, a zero vector will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29ddde2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vectors for all the tokens in s = \"Hello, How are you?\"\n",
    "# Look up \"get_vecs_by_tokens\" for GloVe vectors.\n",
    "# Add an assertion checking that the dimensions of wat you get is dimension (???, 300).\n",
    "assert(GLOVE.get_vecs_by_tokens(\"Hello, How are you?\".split()).shape == (4, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df54d06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.2720, -0.0620, -0.1884,  ...,  0.1302, -0.1832,  0.1323],\n",
       "        [-0.1731,  0.2066,  0.0165,  ...,  0.1666, -0.3834, -0.0738],\n",
       "        [-0.5131, -0.0228, -0.0271,  ..., -0.1723, -0.0968, -0.1644],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let s = \"\"<pad> <unk> the man Man ahsdhashdahsdhash\".\n",
    "# What are the vectors of each token. Print this below.\n",
    "s = \"<pad> <unk> the man Man ahsdhashdahsdhash\"\n",
    "GLOVE.get_vecs_by_tokens(s.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d5cc95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "200b05fc",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6391c2a8",
   "metadata": {},
   "source": [
    "These functions tokenize the string input and then map each token to the integer representation in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16ca1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return for a sentence the int tokens for that sentence.\n",
    "# I.e., you pass in \"a b c d\" and get out [1, 2, 3, 4].\n",
    "def text_pipeline(text):\n",
    "    return [VOCAB[token] for token in TOKENIZER(text)]\n",
    "\n",
    "# Return the label starting at 0. I.e. map each label to fo from 0, not 2 or whatever it starts from.\n",
    "def label_pipeline(label):\n",
    "    return label - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef6734",
   "metadata": {},
   "source": [
    "Nice link on collate_fn and DataLoader in PyTorch: https://python.plainenglish.io/understanding-collate-fn-in-pytorch-f9d1742647d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff479986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we loop through batches, this function gets applied to each raw batch.\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for i in batch:\n",
    "        # Get the label from {1, 2, 3, 4} to {0, 1, 2, 3}\n",
    "        # Append the label to the label_list.\n",
    "        label_list.append(label_pipeline(i[0]))\n",
    "                \n",
    "        # Return a list of ints.\n",
    "        # Get a torch tensor of the sentence, this sould be a tensor of torch.int64.\n",
    "        processed_text = torch.tensor(text_pipeline(i[1]))\n",
    "        text_list.append(processed_text.clone().detach())\n",
    "    \n",
    "    # Transform the label_list into a tensor. \n",
    "    label_list = torch.tensor(label_list)\n",
    "    # Pad the list of text_list tensors so they all have the same length.\n",
    "    # Use batch_first = True.\n",
    "    # Use padding_valid = PADDING_VALUE\n",
    "    text_list = pad_sequence(text_list, batch_first=True, padding_value=PADDING_VALUE)\n",
    "            \n",
    "    return label_list.to(DEVICE), text_list.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1287d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7fcf425",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e617ddce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of classes is 4\n"
     ]
    }
   ],
   "source": [
    "train_iter = DATASETS[DATASET](root=DATA_DIR, split=\"train\")\n",
    "# Get the number of classes.\n",
    "class_ = set()\n",
    "for i in train_iter:\n",
    "    class_.add(i[0])\n",
    "num_class = len(class_)\n",
    "# What are the classes?\n",
    "print(f\"The number of classes is {num_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7071d5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB.get_itos()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa8a40d",
   "metadata": {},
   "source": [
    "### Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc51c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more complicated model. We'll explore this after we learn word embeddings.\n",
    "\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embed_dim,\n",
    "        num_class,\n",
    "        initialize_with_glove = True,\n",
    "        fine_tune_embeddings = True\n",
    "    ):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        # Set to an embedding of (vocab_size, embed_dim) size.\n",
    "        # Use padding_idx = PADDING_IDX.\n",
    "        # This is so we don't get gradients for padding tokens and use 0 as the vector for these.\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embed_dim,\n",
    "            padding_idx=PADDING_IDX\n",
    "        )\n",
    "        \n",
    "        if initialize_with_glove:\n",
    "            # Turn off the gradient for the embedding weight as we are going to modify it. \n",
    "            with torch.no_grad(): \n",
    "                for i in range(vocab_size):\n",
    "                    # Get the token index in VOCAB.\n",
    "                    token = VOCAB.get_itos()[i]\n",
    "                    \n",
    "                    # Modify the embedding matrix to be the GloVe vector for this token.\n",
    "                    self.embedding.weight[i, :] = GLOVE.get_vecs_by_tokens(token)\n",
    "                torch.enable_grad()\n",
    "                # Turn on the gradient after we modify it.\n",
    "                # You could do this in another way by wrapping this in @torch.no_grad decorator.\n",
    "            \n",
    "        \n",
    "        # No fine tuning means once you intialize, these are constant.\n",
    "        if not fine_tune_embeddings:\n",
    "            # Turn off the gradient for the embedding weight matric if you don't fine tune them.\n",
    "            #torch.no_grad()\n",
    "            self.embedding.requires_grad_ = False\n",
    "        \n",
    "        \n",
    "        # Set fc to be a linear layer of dimension (embed_dim, num_class).\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text):\n",
    "        # Get the embeddings for all tokens in the batch of text.\n",
    "        embedded = self.embedding(text)\n",
    "        # Across dimension 1, get the mean vector. This gets the mean vector per sentence in the batch.\n",
    "        # Make sure you squeeze any dimension that's 1. This should be (N, d), where N is the batch dimension and d is the word vector dimension.\n",
    "        embedded_sum = torch.mean(embedded, dim=1).squeeze(1)\n",
    "        \n",
    "        # Run through a linear layer self.fc and also apply ReLU.\n",
    "        relu = nn.ReLU()\n",
    "        logits = relu(self.fc(embedded_sum))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25775647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b3c6ed5",
   "metadata": {},
   "source": [
    "### Set up the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cef585f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to be the CrossEntropyLoss.\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Set model to the TextClassification model.\n",
    "# Turn on intialize_with_glove ad fine_tune_embeddings.\n",
    "model = TextClassificationModel(num_class=num_class, vocab_size=len(VOCAB), embed_dim=EMBED_DIM, initialize_with_glove=True, fine_tune_embeddings=True)\n",
    "\n",
    "# Set the optimizer for SGD with learning rate LR. The parameters are model.parameters.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "# Schedule the learning rate decay to go down each epoch by 1/10.\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a642bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26266d8a",
   "metadata": {},
   "source": [
    "### Set up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c0aebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = DATASETS[DATASET]()\n",
    "# This puts things in a nice format.\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "# Set num_train to 95% length of train_dataset.\n",
    "# This should be an integer.\n",
    "num_train = int(0.95 * len(train_dataset))\n",
    "# The array below should have 2 ints in it, num_train, and the 5% left over for validation.\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train,len(train_dataset) - num_train])\n",
    "\n",
    "# Set to a DataLoader on the training data with batch_size BATCH_SIZE and specify collate_batch.\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85773616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86476e2a",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24950481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(text)\n",
    "        \n",
    "        # Get the loss.\n",
    "        loss = criterion(logits.float(),label)\n",
    "        \n",
    "        # Do back propagation.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the gradients at 0.1\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        \n",
    "        # Do an optimization step.\n",
    "        optimizer.step()\n",
    "        # Get the accuracy for this batch.\n",
    "        total_acc += (logits.argmax(1) == label).sum()\n",
    "        # Get the number of rows in this batch. Use labels.\n",
    "        total_count += len(label)\n",
    "        \n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(epoch, idx, len(dataloader), total_acc / total_count)\n",
    "            )\n",
    "            total_acc, total_count = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ee41b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 2, 2, 3, 2, 3, 3, 3, 1, 3, 1, 1, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "for idx, (label, text) in enumerate(train_dataloader):\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39a702be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, model):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            logits = model(text)\n",
    "            total_acc += (logits.argmax(1) == label).sum() \n",
    "            total_count += len(label)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9e02c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 7125 batches | accuracy    0.296\n",
      "| epoch   1 |  1000/ 7125 batches | accuracy    0.423\n",
      "| epoch   1 |  1500/ 7125 batches | accuracy    0.533\n",
      "| epoch   1 |  2000/ 7125 batches | accuracy    0.590\n",
      "| epoch   1 |  2500/ 7125 batches | accuracy    0.600\n",
      "| epoch   1 |  3000/ 7125 batches | accuracy    0.620\n",
      "| epoch   1 |  3500/ 7125 batches | accuracy    0.623\n",
      "| epoch   1 |  4000/ 7125 batches | accuracy    0.636\n",
      "| epoch   1 |  4500/ 7125 batches | accuracy    0.813\n",
      "| epoch   1 |  5000/ 7125 batches | accuracy    0.855\n",
      "| epoch   1 |  5500/ 7125 batches | accuracy    0.861\n",
      "| epoch   1 |  6000/ 7125 batches | accuracy    0.863\n",
      "| epoch   1 |  6500/ 7125 batches | accuracy    0.872\n",
      "| epoch   1 |  7000/ 7125 batches | accuracy    0.871\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 121.23s | valid accuracy    0.881 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 7125 batches | accuracy    0.892\n",
      "| epoch   2 |  1000/ 7125 batches | accuracy    0.886\n",
      "| epoch   2 |  1500/ 7125 batches | accuracy    0.893\n",
      "| epoch   2 |  2000/ 7125 batches | accuracy    0.896\n",
      "| epoch   2 |  2500/ 7125 batches | accuracy    0.889\n",
      "| epoch   2 |  3000/ 7125 batches | accuracy    0.896\n",
      "| epoch   2 |  3500/ 7125 batches | accuracy    0.899\n",
      "| epoch   2 |  4000/ 7125 batches | accuracy    0.904\n",
      "| epoch   2 |  4500/ 7125 batches | accuracy    0.898\n",
      "| epoch   2 |  5000/ 7125 batches | accuracy    0.905\n",
      "| epoch   2 |  5500/ 7125 batches | accuracy    0.903\n",
      "| epoch   2 |  6000/ 7125 batches | accuracy    0.905\n",
      "| epoch   2 |  6500/ 7125 batches | accuracy    0.906\n",
      "| epoch   2 |  7000/ 7125 batches | accuracy    0.902\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 121.95s | valid accuracy    0.892 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 7125 batches | accuracy    0.897\n",
      "| epoch   3 |  1000/ 7125 batches | accuracy    0.892\n",
      "| epoch   3 |  1500/ 7125 batches | accuracy    0.897\n",
      "| epoch   3 |  2000/ 7125 batches | accuracy    0.900\n",
      "| epoch   3 |  2500/ 7125 batches | accuracy    0.896\n",
      "| epoch   3 |  3000/ 7125 batches | accuracy    0.900\n",
      "| epoch   3 |  3500/ 7125 batches | accuracy    0.900\n",
      "| epoch   3 |  4000/ 7125 batches | accuracy    0.910\n",
      "| epoch   3 |  4500/ 7125 batches | accuracy    0.902\n",
      "| epoch   3 |  5000/ 7125 batches | accuracy    0.908\n",
      "| epoch   3 |  5500/ 7125 batches | accuracy    0.907\n",
      "| epoch   3 |  6000/ 7125 batches | accuracy    0.908\n",
      "| epoch   3 |  6500/ 7125 batches | accuracy    0.910\n",
      "| epoch   3 |  7000/ 7125 batches | accuracy    0.908\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 120.17s | valid accuracy    0.892 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 7125 batches | accuracy    0.898\n",
      "| epoch   4 |  1000/ 7125 batches | accuracy    0.892\n",
      "| epoch   4 |  1500/ 7125 batches | accuracy    0.898\n",
      "| epoch   4 |  2000/ 7125 batches | accuracy    0.900\n",
      "| epoch   4 |  2500/ 7125 batches | accuracy    0.897\n",
      "| epoch   4 |  3000/ 7125 batches | accuracy    0.901\n",
      "| epoch   4 |  3500/ 7125 batches | accuracy    0.900\n",
      "| epoch   4 |  4000/ 7125 batches | accuracy    0.910\n",
      "| epoch   4 |  4500/ 7125 batches | accuracy    0.903\n",
      "| epoch   4 |  5000/ 7125 batches | accuracy    0.909\n",
      "| epoch   4 |  5500/ 7125 batches | accuracy    0.908\n",
      "| epoch   4 |  6000/ 7125 batches | accuracy    0.909\n",
      "| epoch   4 |  6500/ 7125 batches | accuracy    0.910\n",
      "| epoch   4 |  7000/ 7125 batches | accuracy    0.908\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 121.00s | valid accuracy    0.892 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 7125 batches | accuracy    0.898\n",
      "| epoch   5 |  1000/ 7125 batches | accuracy    0.893\n",
      "| epoch   5 |  1500/ 7125 batches | accuracy    0.898\n",
      "| epoch   5 |  2000/ 7125 batches | accuracy    0.900\n",
      "| epoch   5 |  2500/ 7125 batches | accuracy    0.897\n",
      "| epoch   5 |  3000/ 7125 batches | accuracy    0.901\n",
      "| epoch   5 |  3500/ 7125 batches | accuracy    0.900\n",
      "| epoch   5 |  4000/ 7125 batches | accuracy    0.910\n",
      "| epoch   5 |  4500/ 7125 batches | accuracy    0.903\n",
      "| epoch   5 |  5000/ 7125 batches | accuracy    0.909\n",
      "| epoch   5 |  5500/ 7125 batches | accuracy    0.908\n",
      "| epoch   5 |  6000/ 7125 batches | accuracy    0.909\n",
      "| epoch   5 |  6500/ 7125 batches | accuracy    0.910\n",
      "| epoch   5 |  7000/ 7125 batches | accuracy    0.908\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 120.07s | valid accuracy    0.893 \n",
      "-----------------------------------------------------------\n",
      "Checking the results of test dataset.\n",
      "test accuracy    0.889\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader, model, optimizer, criterion, epoch)\n",
    "    accu_val = evaluate(valid_dataloader, model)\n",
    "    scheduler.step()\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(epoch, time.time() - epoch_start_time, accu_val)\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "print(\"Checking the results of test dataset.\")\n",
    "accu_test = evaluate(test_dataloader, model)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a276f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x127f1d370>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 08:28:41) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1cb4ba5f411cfa4a68a7ea6c2f9ba3655e2604bd37447d058a856eda531fd15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
