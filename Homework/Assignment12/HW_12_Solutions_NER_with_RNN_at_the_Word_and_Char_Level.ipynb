{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 177,
      "id": "f13ded9f",
      "metadata": {
        "id": "f13ded9f"
      },
      "outputs": [],
      "source": [
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "%matplotlib inline\n",
        "import collections\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import zipfile\n",
        "from matplotlib import pylab\n",
        "from six.moves import range\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer, ngrams_iterator\n",
        "from torchtext.datasets import DATASETS\n",
        "from torchtext.utils import download_from_url\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torch.nn as nn\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "from torchtext.vocab import FastText, CharNGram\n",
        "from itertools import chain\n",
        "\n",
        "seed = 54321"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1edcf51b",
      "metadata": {
        "id": "1edcf51b"
      },
      "source": [
        "### Download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "id": "4eea599c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eea599c",
        "outputId": "680df7e6-4ffc-481d-c1dd-150368ccc460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found and verified data/conllpp_train.txt\n",
            "Found and verified data/conllpp_dev.txt\n",
            "Found and verified data/conllpp_test.txt\n"
          ]
        }
      ],
      "source": [
        "url = 'https://github.com/ZihanWangKi/CrossWeigh/raw/master/data/'\n",
        "dir_name = 'data'\n",
        "#https://github.com/ZihanWangKi/CrossWeigh/raw/master/data/conllpp_train.txt\n",
        "def download_data(url, filename, download_dir, expected_bytes):\n",
        "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
        "      \n",
        "    # Create directories if doesn't exist\n",
        "    os.makedirs(download_dir, exist_ok=True)\n",
        "    \n",
        "    # If file doesn't exist download\n",
        "    if not os.path.exists(os.path.join(download_dir,filename)):\n",
        "        filepath, _ = urlretrieve(url + filename, os.path.join(download_dir,filename))\n",
        "    else:\n",
        "        filepath = os.path.join(download_dir, filename)\n",
        "    \n",
        "    # Check the file size\n",
        "    statinfo = os.stat(filepath)\n",
        "    if statinfo.st_size == expected_bytes:\n",
        "        print('Found and verified %s' % filepath)\n",
        "    else:\n",
        "        print(statinfo.st_size)\n",
        "        raise Exception(\n",
        "          'Failed to verify ' + filepath + '. Can you get to it with a browser?')\n",
        "        \n",
        "    return filepath\n",
        "\n",
        "# Filepaths to train/valid/test data\n",
        "train_filepath = download_data(url, 'conllpp_train.txt', dir_name, 3283420)\n",
        "dev_filepath = download_data(url, 'conllpp_dev.txt', dir_name, 827443)\n",
        "test_filepath = download_data(url, 'conllpp_test.txt', dir_name, 748737)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "id": "dc3af9ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc3af9ed",
        "outputId": "71fb8fbe-ba5d-4743-d5f9-d631d08b330f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-DOCSTART- -X- -X- O\n",
            "\n",
            "EU NNP B-NP B-ORG\n",
            "rejects VBZ B-VP O\n",
            "German JJ B-NP B-MISC\n",
            "call NN I-NP O\n",
            "to TO B-VP O\n",
            "boycott VB I-VP O\n",
            "British JJ B-NP B-MISC\n",
            "lamb NN I-NP O\n"
          ]
        }
      ],
      "source": [
        "!head data/conllpp_train.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4203ce6",
      "metadata": {
        "id": "e4203ce6"
      },
      "source": [
        "### Read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "id": "4db2c06f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4db2c06f",
        "outputId": "2f696b31-864f-4629-ec5d-f059e3b5659b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data ...\n",
            "\tDone\n",
            "Reading data ...\n",
            "\tDone\n",
            "Reading data ...\n",
            "\tDone\n",
            "Train size: 14041\n",
            "Valid size: 3250\n",
            "Test size: 3452\n",
            "\n",
            "Sample data\n",
            "\n",
            "Sentence: CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY .\n",
            "Labels: ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "\n",
            "\n",
            "Sentence: LONDON 1996-08-30\n",
            "Labels: ['B-LOC', 'O']\n",
            "\n",
            "\n",
            "Sentence: West Indian all-rounder Phil Simmons took four for 38 on Friday as Leicestershire beat Somerset by an innings and 39 runs in two days to take over at the head of the county championship .\n",
            "Labels: ['B-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "\n",
            "\n",
            "Sentence: Their stay on top , though , may be short-lived as title rivals Essex , Derbyshire and Surrey all closed in on victory while Kent made up for lost time in their rain-affected match against Nottinghamshire .\n",
            "Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O']\n",
            "\n",
            "\n",
            "Sentence: After bowling Somerset out for 83 on the opening morning at Grace Road , Leicestershire extended their first innings by 94 runs before being bowled out for 296 with England discard Andy Caddick taking three for 83 .\n",
            "Labels: ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O']\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def read_data(filename):\n",
        "    '''\n",
        "    Read data from a file with given filename\n",
        "    Returns a list of sentences (each sentence a string), \n",
        "    and list of ner labels for each string\n",
        "    '''\n",
        "\n",
        "    print(\"Reading data ...\")\n",
        "    # master lists - Holds sentences (list of tokens), ner_labels (for each token an NER label)\n",
        "    sentences, ner_labels = [], [] \n",
        "    \n",
        "    # Open the file\n",
        "    with open(filename,'r',encoding='latin-1') as f:        \n",
        "        # Read each line\n",
        "        is_sos = True # We record at each line if we are seeing the beginning of a sentence\n",
        "        \n",
        "        # Tokens and labels of a single sentence, flushed when encountered a new one\n",
        "        sentence_tokens = []\n",
        "        sentence_labels = []\n",
        "        i = 0\n",
        "        for row in f:\n",
        "            # If we are seeing an empty line or -DOCSTART- that's a new line\n",
        "            if len(row.strip()) == 0 or row.split(' ')[0] == '-DOCSTART-':\n",
        "                is_sos = False\n",
        "            # Otherwise keep capturing tokens and labels\n",
        "            else:\n",
        "                is_sos = True\n",
        "                token, _, _, ner_tag = row.split(' ')\n",
        "                sentence_tokens.append(token)\n",
        "                sentence_labels.append(ner_tag.strip())\n",
        "            \n",
        "            # When we reach the end / or reach the beginning of next\n",
        "            # add the data to the master lists, flush the temporary one\n",
        "            if not is_sos and len(sentence_tokens)>0:\n",
        "                sentences.append(' '.join(sentence_tokens))\n",
        "                ner_labels.append(sentence_labels)\n",
        "                sentence_tokens, sentence_labels = [], []\n",
        "    \n",
        "    print('\\tDone')\n",
        "    return sentences, ner_labels\n",
        "\n",
        "# Train data\n",
        "train_sentences, train_labels = read_data(train_filepath) \n",
        "# Validation data\n",
        "valid_sentences, valid_labels = read_data(dev_filepath) \n",
        "# Test data\n",
        "test_sentences, test_labels = read_data(test_filepath) \n",
        "\n",
        "# Print some stats\n",
        "print(f\"Train size: {len(train_labels)}\")\n",
        "print(f\"Valid size: {len(valid_labels)}\")\n",
        "print(f\"Test size: {len(test_labels)}\")\n",
        "\n",
        "# Print some data\n",
        "print('\\nSample data\\n')\n",
        "for v_sent, v_labels in zip(valid_sentences[:5], valid_labels[:5]):\n",
        "    print(f\"Sentence: {v_sent}\")\n",
        "    print(f\"Labels: {v_labels}\")\n",
        "    assert(len(v_sent.split(' ')) == len(v_labels))\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert(len(train_labels) == 14041)\n",
        "assert(len(valid_labels) == 3250)\n",
        "assert(len(test_labels) == 3452)"
      ],
      "metadata": {
        "id": "o4gWY5fwOggZ"
      },
      "id": "o4gWY5fwOggZ",
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "id": "f25d7ec1",
      "metadata": {
        "id": "f25d7ec1"
      },
      "outputs": [],
      "source": [
        "# We build these since the basic english tokenizer does get rid of some tokens that are useful\n",
        "# Lowercase everything to make this simplier\n",
        "class SentenceTokenizer():\n",
        "    def __call__(self, sentence):\n",
        "        return sentence.lower().split(' ')\n",
        "    \n",
        "class WordTokenizer():\n",
        "    def __call__(self, word):\n",
        "        return [c for c in word.lower()]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SENTENCE_TOKENIZER = SentenceTokenizer()\n",
        "WORD_TOKENIZER = WordTokenizer()"
      ],
      "metadata": {
        "id": "zWiq0llwObJ4"
      },
      "id": "zWiq0llwObJ4",
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "id": "d4a21863",
      "metadata": {
        "id": "d4a21863"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "id": "29dce365",
      "metadata": {
        "id": "29dce365"
      },
      "outputs": [],
      "source": [
        "assert(len(WORD_TOKENIZER(\"this is a sentence\")) == 18)\n",
        "assert(len(SENTENCE_TOKENIZER(\"this is a sentence\")) == 4)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "id": "e6e3a1c9",
      "metadata": {
        "id": "e6e3a1c9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "id": "2eee65e3",
      "metadata": {
        "id": "2eee65e3"
      },
      "outputs": [],
      "source": [
        "sentences = train_sentences + test_sentences + valid_sentences\n",
        "labels = train_labels + test_labels + valid_labels\n",
        "\n",
        "def yield_word_tokens(sentences):\n",
        "    for sentence in sentences:\n",
        "        word_tokens = SENTENCE_TOKENIZER(sentence)\n",
        "        # A list of word tokens.\n",
        "        yield word_tokens\n",
        "        \n",
        "def yield_char_tokens(sentences):\n",
        "    for word_tokens in yield_word_tokens(sentences):\n",
        "        for word_token in word_tokens:\n",
        "            char_tokens = WORD_TOKENIZER(word_token)\n",
        "            yield char_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "id": "b2861dab",
      "metadata": {
        "id": "b2861dab"
      },
      "outputs": [],
      "source": [
        "WORD_VOCAB = build_vocab_from_iterator(\n",
        "    yield_word_tokens(sentences),\n",
        "    specials=('<pad>', '<unk>')\n",
        ")\n",
        "\n",
        "CHAR_VOCAB = build_vocab_from_iterator(\n",
        "    yield_char_tokens(sentences),\n",
        "    specials=('<pad>', '<unk>')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "id": "e3993201",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3993201",
        "outputId": "c57a7c0b-c41b-4b99-a28c-af3b828ef07b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[64, 31, 8, 1780]"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ],
      "source": [
        "# Example: You should see 4 integer tokens below.\n",
        "WORD_VOCAB(SENTENCE_TOKENIZER(\"this is a sentence\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: You should see 4 integer tokens below.\n",
        "CHAR_VOCAB(WORD_TOKENIZER(\"Xhis\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRM6lmVtoTWf",
        "outputId": "3ce3936c-7fe7-4a0e-8934-d026833b177f"
      },
      "id": "QRM6lmVtoTWf",
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[42, 12, 6, 8]"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "id": "9c1e86cd",
      "metadata": {
        "id": "9c1e86cd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "id": "f5dc348d",
      "metadata": {
        "id": "f5dc348d"
      },
      "outputs": [],
      "source": [
        "# Get the word to idx and idx to char dictionaries\n",
        "wtoi = WORD_VOCAB.get_stoi()\n",
        "itow = WORD_VOCAB.get_itos()\n",
        "# Get the char to idx and idx to char dictionaries\n",
        "ctoi = CHAR_VOCAB.get_stoi()\n",
        "itoc = CHAR_VOCAB.get_itos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "id": "2abb364f",
      "metadata": {
        "id": "2abb364f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46f065d7-63cc-4ea3-f9ea-2d9149823aa0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'#': 60,\n",
              " '`': 59,\n",
              " 'y': 21,\n",
              " 'b': 20,\n",
              " 'w': 19,\n",
              " 'g': 18,\n",
              " '!': 58,\n",
              " 'f': 17,\n",
              " 'm': 15,\n",
              " 'c': 13,\n",
              " '<unk>': 1,\n",
              " 'o': 7,\n",
              " 'v': 27,\n",
              " '&': 51,\n",
              " 'p': 16,\n",
              " 'l': 10,\n",
              " '+': 48,\n",
              " ':': 45,\n",
              " 'e': 2,\n",
              " 'a': 3,\n",
              " '5': 33,\n",
              " '-': 24,\n",
              " '<pad>': 0,\n",
              " '/': 46,\n",
              " 'i': 6,\n",
              " 'k': 26,\n",
              " '=': 52,\n",
              " 'n': 5,\n",
              " 'd': 11,\n",
              " '\"': 40,\n",
              " '1': 23,\n",
              " ',': 25,\n",
              " 'u': 14,\n",
              " '*': 50,\n",
              " '6': 31,\n",
              " 't': 4,\n",
              " 'r': 9,\n",
              " '3': 32,\n",
              " '2': 29,\n",
              " '9': 30,\n",
              " '4': 34,\n",
              " '8': 35,\n",
              " ']': 56,\n",
              " '7': 36,\n",
              " '0': 28,\n",
              " '(': 37,\n",
              " 'h': 12,\n",
              " 's': 8,\n",
              " 'j': 39,\n",
              " \"'\": 41,\n",
              " '$': 47,\n",
              " '.': 22,\n",
              " 'x': 42,\n",
              " 'z': 43,\n",
              " 'q': 44,\n",
              " ';': 49,\n",
              " '%': 53,\n",
              " '?': 54,\n",
              " ')': 38,\n",
              " '[': 55,\n",
              " '@': 57}"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ],
      "source": [
        "ctoi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "id": "081c5ee8",
      "metadata": {
        "id": "081c5ee8"
      },
      "outputs": [],
      "source": [
        "assert(len(wtoi) == 26871)\n",
        "assert(len(ctoi) == 61)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "id": "f5398c46",
      "metadata": {
        "id": "f5398c46"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "id": "ac468388",
      "metadata": {
        "id": "ac468388",
        "outputId": "2c44e8b0-f65c-47f0-c4cf-99937352de3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ],
      "source": [
        "# You should see 0 and 0 below\n",
        "WORD_VOCAB['<pad>'], CHAR_VOCAB['<pad>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "id": "d4227a0f",
      "metadata": {
        "id": "d4227a0f",
        "outputId": "6c453027-c9b9-4180-dcee-daaf9bbffbfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ],
      "source": [
        "# You should see 1 and 1 below\n",
        "WORD_VOCAB['<unk>'], CHAR_VOCAB['<unk>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "id": "931d9064",
      "metadata": {
        "id": "931d9064"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "id": "d81a25c9",
      "metadata": {
        "id": "d81a25c9"
      },
      "outputs": [],
      "source": [
        "# We need to carefully weight all the classes \n",
        "# We use w(c) = min(freq(l)) / freq(c); lower frequency classes \n",
        "# So a low class gets a weight that's higher, a higher class a lower weight\n",
        "def get_label_id_map(labels):\n",
        "    # Get the unique list of labels\n",
        "    unique_labels = pd.Series(chain(*labels)).unique()\n",
        "    # Create a dictionary label to idx, starting with idx 0\n",
        "    ltoi = dict(zip(unique_labels, np.arange(unique_labels.shape[0])))\n",
        "    # Make a map from idx to label\n",
        "    itol = {i : label for label, i in ltoi.items()}\n",
        "    pd.Series(chain(*labels))\n",
        "    \n",
        "    itolw = {}\n",
        "    \n",
        "    label_to_count = pd.Series(chain(*labels)).value_counts()\n",
        "    \n",
        "    for label, count in label_to_count.items():\n",
        "        itolw[ltoi[label]] = label_to_count.min() / count\n",
        "    \n",
        "    # Return (ltoi, itol, itolw)\n",
        "    return ltoi, itol, itolw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "id": "546db172",
      "metadata": {
        "id": "546db172"
      },
      "outputs": [],
      "source": [
        "assert(len(pd.Series(chain(*train_labels)).unique()) == 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "id": "fa3b9fc5",
      "metadata": {
        "id": "fa3b9fc5"
      },
      "outputs": [],
      "source": [
        "ltoi, itol, itolw = get_label_id_map(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for l, idx in ltoi.items():\n",
        "  assert(l == itol[idx])\n",
        "  assert(idx in itolw)"
      ],
      "metadata": {
        "id": "KMsQOeBSguWx"
      },
      "id": "KMsQOeBSguWx",
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "id": "a9494010",
      "metadata": {
        "id": "a9494010",
        "outputId": "89ae1b5b-6908-4bf5-f609-2d37a6bee38b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 0.006811025015037328,\n",
              " 5: 0.16176470588235295,\n",
              " 3: 0.175,\n",
              " 0: 0.18272425249169436,\n",
              " 4: 0.25507950530035334,\n",
              " 6: 0.31182505399568033,\n",
              " 2: 0.33595113438045376,\n",
              " 8: 0.9982713915298185,\n",
              " 7: 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ],
      "source": [
        "# Look at the weights per tag\n",
        "itolw"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ltoi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5EtHfsQg7mB",
        "outputId": "23cd8578-114c-4060-d725-56291658c7a7"
      },
      "id": "F5EtHfsQg7mB",
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-ORG': 0,\n",
              " 'O': 1,\n",
              " 'B-MISC': 2,\n",
              " 'B-PER': 3,\n",
              " 'I-PER': 4,\n",
              " 'B-LOC': 5,\n",
              " 'I-ORG': 6,\n",
              " 'I-MISC': 7,\n",
              " 'I-LOC': 8}"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert(min(itolw.values()) == 0.006811025015037328)"
      ],
      "metadata": {
        "id": "BsJx4RzDRvk5"
      },
      "id": "BsJx4RzDRvk5",
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "id": "96393bb8",
      "metadata": {
        "id": "96393bb8"
      },
      "outputs": [],
      "source": [
        "# Get the weights per class as a tensor\n",
        "weights = torch.zeros(len(itolw))\n",
        "for i, lw in itolw.items():\n",
        "    weights[i] = lw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "id": "aaf2e8d9",
      "metadata": {
        "id": "aaf2e8d9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "id": "4b964cf9",
      "metadata": {
        "id": "4b964cf9"
      },
      "outputs": [],
      "source": [
        "labels = pd.Series(chain(*train_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1hus59XV0Fy",
        "outputId": "fb309794-c0b9-483f-d974-e844025911bc"
      },
      "id": "r1hus59XV0Fy",
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0          B-ORG\n",
            "1              O\n",
            "2         B-MISC\n",
            "3              O\n",
            "4              O\n",
            "           ...  \n",
            "203616         O\n",
            "203617     B-ORG\n",
            "203618         O\n",
            "203619     B-ORG\n",
            "203620         O\n",
            "Length: 203621, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "id": "80c8bf01",
      "metadata": {
        "id": "80c8bf01",
        "outputId": "e34dbe56-4253-48e3-c432-1bf6e8a9625e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O 169578\n",
            "B-LOC 7140\n",
            "B-PER 6600\n",
            "B-ORG 6321\n",
            "I-PER 4528\n",
            "I-ORG 3704\n",
            "B-MISC 3438\n",
            "I-LOC 1157\n",
            "I-MISC 1155\n"
          ]
        }
      ],
      "source": [
        "for k, v in labels.value_counts().items():\n",
        "    print(k, v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "id": "8f2546c1",
      "metadata": {
        "id": "8f2546c1"
      },
      "outputs": [],
      "source": [
        "assert(labels.value_counts().min() == 1155)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "id": "ebb5a61d",
      "metadata": {
        "id": "ebb5a61d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "bddf6405",
      "metadata": {
        "id": "bddf6405"
      },
      "source": [
        "### Check for class balance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "id": "13522bf0",
      "metadata": {
        "id": "13522bf0",
        "outputId": "13bebb52-0fa4-4fef-994e-871ea3313011",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data label counts\n",
            "O         169578\n",
            "B-LOC       7140\n",
            "B-PER       6600\n",
            "B-ORG       6321\n",
            "I-PER       4528\n",
            "I-ORG       3704\n",
            "B-MISC      3438\n",
            "I-LOC       1157\n",
            "I-MISC      1155\n",
            "dtype: int64\n",
            "\n",
            "Validation data label counts\n",
            "O         42759\n",
            "B-PER      1842\n",
            "B-LOC      1837\n",
            "B-ORG      1341\n",
            "I-PER      1307\n",
            "B-MISC      922\n",
            "I-ORG       751\n",
            "I-MISC      346\n",
            "I-LOC       257\n",
            "dtype: int64\n",
            "\n",
            "Test data label counts\n",
            "O         38143\n",
            "B-ORG      1714\n",
            "B-LOC      1645\n",
            "B-PER      1617\n",
            "I-PER      1161\n",
            "I-ORG       881\n",
            "B-MISC      722\n",
            "I-LOC       259\n",
            "I-MISC      252\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Print the value count for each label\n",
        "print(\"Training data label counts\")\n",
        "print(pd.Series(chain(*train_labels)).value_counts())\n",
        "\n",
        "print(\"\\nValidation data label counts\")\n",
        "print(pd.Series(chain(*valid_labels)).value_counts())\n",
        "\n",
        "print(\"\\nTest data label counts\")\n",
        "print(pd.Series(chain(*test_labels)).value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GVHS06FSWQs3"
      },
      "id": "GVHS06FSWQs3",
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ce51d1f7",
      "metadata": {
        "id": "ce51d1f7"
      },
      "source": [
        "### Series length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "id": "eb6a72e7",
      "metadata": {
        "id": "eb6a72e7",
        "outputId": "a2cb0c89-a262-46a0-d3bd-207bc0bf2571",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    14041.000000\n",
              "mean        14.501887\n",
              "std         11.602756\n",
              "min          1.000000\n",
              "5%           2.000000\n",
              "50%         10.000000\n",
              "95%         37.000000\n",
              "max        113.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ],
      "source": [
        "# Display the mean sentence length for the training samples\n",
        "# You should get around 15 mean ...  What about median, 95%, etc?\n",
        "# .describe applied to a certain series is a good idea ...\n",
        "pd.Series(train_sentences).str.split().str.len().describe(percentiles=[0.05, 0.95])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec9ef51e",
      "metadata": {
        "id": "ec9ef51e"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "id": "dc9dad21",
      "metadata": {
        "id": "dc9dad21"
      },
      "outputs": [],
      "source": [
        "# Size of token embeddings\n",
        "d_model = 300\n",
        "\n",
        "# Number of hidden units in the GRU layer\n",
        "d_hidden = 64\n",
        "\n",
        "# Number of hidden units in the GRU layer\n",
        "d_char = 32\n",
        "\n",
        "# Number of output nodes in the last layer\n",
        "num_classes = len(itol)\n",
        "\n",
        "# Number of samples in a batch\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# Number of training epochs.\n",
        "EPOCHS = 25\n",
        "\n",
        "# FastText embeddings\n",
        "FAST_TEXT = FastText(\"simple\")\n",
        "\n",
        "# Learning rate\n",
        "LR = 1.0\n",
        "\n",
        "# Get the weights per class\n",
        "weight = weights\n",
        "\n",
        "# Maximum word length; critical for convolutions\n",
        "MAX_WORD_LENGTH = 12\n",
        "\n",
        "# The device to run on\n",
        "# Change this to 'mps' if you are on a mac with MPS\n",
        "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "id": "380ba279",
      "metadata": {
        "id": "380ba279"
      },
      "outputs": [],
      "source": [
        "assert(len(train_sentences) // BATCH_SIZE == 109)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "id": "23b33852",
      "metadata": {
        "id": "23b33852"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "id": "7e6822d8",
      "metadata": {
        "id": "7e6822d8"
      },
      "outputs": [],
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, sentence_list, sentence_lengths = [], [], []\n",
        "    word_list = []\n",
        "    # The sentence below is already transformed to int tokens.\n",
        "    for sentence, words, labels in batch:\n",
        "        sentence_list.append(torch.tensor(\n",
        "            sentence, \n",
        "            dtype=torch.int64\n",
        "        ))\n",
        "        sentence_lengths.append(len(sentence))\n",
        "        label_list.append(\n",
        "            torch.tensor(\n",
        "                labels, \n",
        "                dtype=torch.int64\n",
        "            )\n",
        "        )\n",
        "        word_list.append(\n",
        "           torch.tensor(\n",
        "            words, \n",
        "            dtype=torch.int64\n",
        "        ))\n",
        "            \n",
        "    return (\n",
        "        # (N, L_sentence)\n",
        "        nn.utils.rnn.pad_sequence(\n",
        "            sentence_list,\n",
        "            batch_first=True\n",
        "        ).to(DEVICE),\n",
        "        nn.utils.rnn.pad_sequence(\n",
        "            label_list,\n",
        "            batch_first=True,\n",
        "            padding_value=-1 # This is not like the vocabulary, this will be ignored by the loss in particular\n",
        "        ).to(DEVICE),\n",
        "        torch.tensor(sentence_lengths).to(DEVICE),\n",
        "        # (N, L_sentence, L_word) where L_word (max) = 12\n",
        "        # This is padded at the word level, but not sentence level\n",
        "        nn.utils.rnn.pad_sequence( \n",
        "            word_list,\n",
        "            batch_first=True\n",
        "        ).to(DEVICE)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "id": "0a141dd7",
      "metadata": {
        "id": "0a141dd7"
      },
      "outputs": [],
      "source": [
        "def get_dl(sentences, labels):\n",
        "    \n",
        "    # Maybe sort by the sentences by length so batches have roughly the same data?\n",
        "    \n",
        "    data = []\n",
        "    \n",
        "    # Note that we need to do our own \n",
        "    for sentence, labels in zip(sentences, labels):\n",
        "        word_tokens = SENTENCE_TOKENIZER(sentence)\n",
        "        int_sentence = WORD_VOCAB(word_tokens)\n",
        "        int_words = []\n",
        "        for word_token in word_tokens:\n",
        "            int_words.append(\n",
        "                CHAR_VOCAB(\n",
        "                    WORD_TOKENIZER(word_token[:MAX_WORD_LENGTH]) + max(0, MAX_WORD_LENGTH - len(word_token)) * ['<pad>']\n",
        "                )\n",
        "            )\n",
        "                    \n",
        "        labels = [ltoi[label] for label in labels]\n",
        "        assert(len(int_sentence) == len(labels))\n",
        "        data.append([int_sentence, int_words, labels])\n",
        "        \n",
        "    return DataLoader(\n",
        "        data,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_batch\n",
        "    )    \n",
        "\n",
        "train_dl = get_dl(train_sentences, train_labels)\n",
        "valid_dl = get_dl(valid_sentences, valid_labels)\n",
        "test_dl = get_dl(test_sentences, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "id": "cfda3844",
      "metadata": {
        "id": "cfda3844"
      },
      "outputs": [],
      "source": [
        "assert(len(train_dl) == 110)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "id": "8c0f1dad",
      "metadata": {
        "id": "8c0f1dad"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "id": "9b308741",
      "metadata": {
        "id": "9b308741"
      },
      "outputs": [],
      "source": [
        "class GRUNERModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_class,\n",
        "        d_model, \n",
        "        d_hidden,\n",
        "        initialize = True,\n",
        "        fine_tune_embeddings = True,\n",
        "        use_conv_embeddings = True,\n",
        "    ):\n",
        "        \n",
        "        super(GRUNERModel, self).__init__()\n",
        "        self.vocab_size = len(WORD_VOCAB)\n",
        "        self.d_model = d_model\n",
        "        self.d_hidden = d_hidden\n",
        "        self.d_char = 32\n",
        "        self.kernel = 5\n",
        "        self.max_word_length = MAX_WORD_LENGTH\n",
        "        self.use_conv_embeddings = use_conv_embeddings\n",
        "        \n",
        "        if self.use_conv_embeddings:\n",
        "            # 12 - 5 + 1 = 8\n",
        "            # Input data will be (N * L_sentence, D_char, L_word = 12)\n",
        "            self.conv = nn.Conv1d(self.d_char, self.d_char, self.kernel)\n",
        "            # Will results in (N * L_sentence, D_char, 8) data.\n",
        "            # D_char is 32.\n",
        "            # Will result is (32, 1) vector for each word.\n",
        "            self.max_pool = nn.MaxPool1d(self.max_word_length - self.kernel + 1)\n",
        "            \n",
        "        self.embedding = nn.Embedding(\n",
        "            len(WORD_VOCAB),\n",
        "            d_model if not initialize else 300, # The FastText embeddings we use below have dimension 300\n",
        "            padding_idx = 0\n",
        "        )\n",
        "        \n",
        "        self.char_embedding = nn.Embedding(\n",
        "            len(CHAR_VOCAB),\n",
        "            self.d_char, \n",
        "            padding_idx = 0\n",
        "        )\n",
        "        \n",
        "        if initialize:\n",
        "            self.embedding.weight.requires_grad = False\n",
        "            for i in range(len(WORD_VOCAB)):\n",
        "                token = WORD_VOCAB.lookup_token(i)\n",
        "                self.embedding.weight[i, :] = FAST_TEXT.get_vecs_by_tokens(\n",
        "                    token, \n",
        "                    lower_case_backup=True\n",
        "                )\n",
        "            self.embedding.weight.requires_grad = True\n",
        "        else:\n",
        "            self.init_weights()\n",
        "                \n",
        "        if not fine_tune_embeddings:\n",
        "            self.embedding.weight.requires_grad = False\n",
        "        \n",
        "        self.rnn = nn.GRU(\n",
        "            self.d_model + self.d_char, \n",
        "            self.d_hidden,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Bidirectional; so we go from 2 * d_hidden to num_class\n",
        "        self.fc = nn.Linear(2 * self.d_hidden, num_class)\n",
        "\n",
        "        # Note: for drop out + ReLu, order does not matters\n",
        "        # Use 0.3 for the dropout probability\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "    def init_weights(self):\n",
        "        # Initialize the word embedding layer with uniform random variables between (-initrange, initrange)\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        if self.use_conv_embeddings:\n",
        "          self.char_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    # N = batch_size,\n",
        "    # L_sentence = sequence length\n",
        "    # D_word = word embedding length\n",
        "    # D_char = char embedding length\n",
        "    # Hout = hidden dimenson from bidirectional GRU\n",
        "    # C = number of classes\n",
        "    def forward(self, sentences, lengths, words):\n",
        "        # (N, L_sentence, D_word)\n",
        "        embedded_sentences = self.embedding(sentences.int()) \n",
        "        \n",
        "        if self.use_conv_embeddings:                        \n",
        "            # (N, L_sentence, L_word, D_char)\n",
        "            embedded_words = self.char_embedding(words.int())\n",
        "                                                \n",
        "            N, L_sentence, L_word, D_char = embedded_words.shape\n",
        "            \n",
        "            # (N * L_sentence, L_word, D_char)\n",
        "            embedded_words = embedded_words.view(N * L_sentence, L_word, -1)\n",
        "\n",
        "            # (N * L_sentence, D_char, L_word)                        \n",
        "            embedded_words = torch.swapaxes(embedded_words, 2, 1)\n",
        "                        \n",
        "            # 12 - 4, since kernel size is 5\n",
        "            # (N * L_sentence, D_char, L_word - kernel_size + 1 )\n",
        "            embedded_words = self.conv(embedded_words)\n",
        "                        \n",
        "            # (N * L_sentence, D_char, 1)\n",
        "            embedded_words = self.max_pool(embedded_words).squeeze()\n",
        "                        \n",
        "            # (N, L_sentence, D_char)\n",
        "            embedded_words = embedded_words.view(N, L_sentence, -1)\n",
        "\n",
        "            #  (N, L_sentence, D_char + D_word)           \n",
        "            embedded_sentences = torch.cat([embedded_sentences, embedded_words], axis=-1)\n",
        "            \n",
        "        # This is a key for efficient computation. \n",
        "        # Pack the padded embeddings. Magic.\n",
        "        embedded_sentences = nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded_sentences,\n",
        "            lengths.cpu().numpy(),\n",
        "            enforce_sorted=False,\n",
        "            batch_first=True\n",
        "        )\n",
        "        \n",
        "        # (N * L_sentence sort of, Hout)\n",
        "        logits, _ = self.rnn(embedded_sentences)\n",
        "        \n",
        "         # (N, L_sentence, Hout) \n",
        "        logits, _ = nn.utils.rnn.pad_packed_sequence(logits, batch_first=True) # (N, L, Hout)\n",
        "\n",
        "        # (N, L, C)\n",
        "        logits = self.fc(logits)\n",
        "        \n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "id": "fb46b149",
      "metadata": {
        "id": "fb46b149"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "id": "ccdb26b8",
      "metadata": {
        "id": "ccdb26b8"
      },
      "outputs": [],
      "source": [
        "# Used so we do not include padding indices.\n",
        "# Also, give different weights to different classes to account for class imbalance.\n",
        "criterion = torch.nn.CrossEntropyLoss(weight = weights, ignore_index=-1).to(DEVICE)\n",
        "\n",
        "model = GRUNERModel(\n",
        "    num_classes,\n",
        "    d_model,\n",
        "    d_hidden,\n",
        "    initialize=True,\n",
        "    fine_tune_embeddings=True,\n",
        "    use_conv_embeddings=True,\n",
        ").to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "id": "09270189",
      "metadata": {
        "id": "09270189"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "id": "7754793c",
      "metadata": {
        "id": "7754793c"
      },
      "outputs": [],
      "source": [
        "from re import escape\n",
        "def train(dl, model, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    total_loss, total_batches = 0.0, 0.0\n",
        "    log_interval = 50\n",
        "\n",
        "    for idx, (sentences, labels, lengths, words) in enumerate(dl):\n",
        "        optimizer.zero_grad()\n",
        "                        \n",
        "        logits = model(sentences, lengths, words)\n",
        "                           \n",
        "        # Get the loss\n",
        "        N, L, _ = logits.shape\n",
        "        logits = logits.view(N * L, -1)\n",
        "        labels = labels.view(N * L)\n",
        "        loss = criterion(input=logits, target=labels)\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        total_batches += 1\n",
        "        \n",
        "        # Do back propagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # Clip the gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        \n",
        "        # Do an optimization step\n",
        "        optimizer.step()\n",
        "        model.eval()\n",
        "\n",
        "        # Get the mask and then find out the predictions\n",
        "        masks = (labels != -1)\n",
        "        total_acc += (logits.argmax(-1) == labels)[masks].sum().item()\n",
        "        total_count += masks.sum()\n",
        "\n",
        "        model.train()\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            print(\n",
        "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
        "                \"| accuracy {:8.3f} \"\n",
        "                \"| loss {:8.3f}\".format(\n",
        "                    epoch,\n",
        "                    idx,\n",
        "                    len(dl),\n",
        "                    total_acc / total_count,\n",
        "                    total_loss / total_batches\n",
        "                )\n",
        "            )\n",
        "            total_acc, total_count = 0, 0\n",
        "            total_loss, total_batches  = 0.0, 0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "apQly1mOtfOe"
      },
      "id": "apQly1mOtfOe",
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "id": "e3cd7dc9",
      "metadata": {
        "id": "e3cd7dc9"
      },
      "outputs": [],
      "source": [
        "def evaluate(dl, model):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "    total_loss, total_batches = 0.0, 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (sentences, labels, lengths, words) in enumerate(dl):\n",
        "            logits = model(sentences, lengths, words)\n",
        "            N, L, _ = logits.shape\n",
        "            logits = logits.view(N * L, -1)\n",
        "            labels = labels.view(N * L)\n",
        "            \n",
        "            total_loss += criterion(input=logits, target=labels)\n",
        "            total_batches += 1\n",
        "            \n",
        "            masks = (labels != -1)\n",
        "            total_acc += (logits.argmax(-1) == labels)[masks].sum().item()\n",
        "            total_count += masks.sum()\n",
        "            \n",
        "    return total_acc / total_count, total_loss / total_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "id": "8b5d9836",
      "metadata": {
        "id": "8b5d9836"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "id": "bb9e1f47",
      "metadata": {
        "id": "bb9e1f47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3da8951-e395-476b-938e-5f64bec14dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |    50/  110 batches | accuracy    0.442 | loss    1.946\n",
            "| epoch   1 |   100/  110 batches | accuracy    0.736 | loss    1.468\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 26.13s | valid accuracy    0.799 | valid loss    1.190 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |    50/  110 batches | accuracy    0.778 | loss    1.183\n",
            "| epoch   2 |   100/  110 batches | accuracy    0.778 | loss    1.161\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 25.50s | valid accuracy    0.791 | valid loss    1.139 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |    50/  110 batches | accuracy    0.782 | loss    1.134\n",
            "| epoch   3 |   100/  110 batches | accuracy    0.782 | loss    1.145\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 25.20s | valid accuracy    0.789 | valid loss    1.129 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |    50/  110 batches | accuracy    0.780 | loss    1.138\n",
            "| epoch   4 |   100/  110 batches | accuracy    0.783 | loss    1.130\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 24.99s | valid accuracy    0.789 | valid loss    1.125 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |    50/  110 batches | accuracy    0.779 | loss    1.119\n",
            "| epoch   5 |   100/  110 batches | accuracy    0.782 | loss    1.137\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 25.57s | valid accuracy    0.788 | valid loss    1.126 \n",
            "-----------------------------------------------------------\n",
            "| epoch   6 |    50/  110 batches | accuracy    0.783 | loss    1.135\n",
            "| epoch   6 |   100/  110 batches | accuracy    0.779 | loss    1.125\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 24.64s | valid accuracy    0.788 | valid loss    1.128 \n",
            "-----------------------------------------------------------\n",
            "| epoch   7 |    50/  110 batches | accuracy    0.780 | loss    1.127\n",
            "| epoch   7 |   100/  110 batches | accuracy    0.780 | loss    1.139\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 24.77s | valid accuracy    0.788 | valid loss    1.125 \n",
            "-----------------------------------------------------------\n",
            "| epoch   8 |    50/  110 batches | accuracy    0.780 | loss    1.128\n",
            "| epoch   8 |   100/  110 batches | accuracy    0.781 | loss    1.134\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 25.71s | valid accuracy    0.788 | valid loss    1.126 \n",
            "-----------------------------------------------------------\n",
            "| epoch   9 |    50/  110 batches | accuracy    0.779 | loss    1.116\n",
            "| epoch   9 |   100/  110 batches | accuracy    0.781 | loss    1.149\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 25.35s | valid accuracy    0.788 | valid loss    1.132 \n",
            "-----------------------------------------------------------\n",
            "| epoch  10 |    50/  110 batches | accuracy    0.781 | loss    1.137\n",
            "| epoch  10 |   100/  110 batches | accuracy    0.781 | loss    1.125\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time: 25.91s | valid accuracy    0.788 | valid loss    1.125 \n",
            "-----------------------------------------------------------\n",
            "| epoch  11 |    50/  110 batches | accuracy    0.782 | loss    1.126\n",
            "| epoch  11 |   100/  110 batches | accuracy    0.780 | loss    1.130\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  11 | time: 25.54s | valid accuracy    0.788 | valid loss    1.128 \n",
            "-----------------------------------------------------------\n",
            "| epoch  12 |    50/  110 batches | accuracy    0.781 | loss    1.132\n",
            "| epoch  12 |   100/  110 batches | accuracy    0.780 | loss    1.136\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  12 | time: 26.02s | valid accuracy    0.788 | valid loss    1.129 \n",
            "-----------------------------------------------------------\n",
            "| epoch  13 |    50/  110 batches | accuracy    0.784 | loss    1.113\n",
            "| epoch  13 |   100/  110 batches | accuracy    0.778 | loss    1.151\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  13 | time: 26.25s | valid accuracy    0.788 | valid loss    1.132 \n",
            "-----------------------------------------------------------\n",
            "| epoch  14 |    50/  110 batches | accuracy    0.781 | loss    1.122\n",
            "| epoch  14 |   100/  110 batches | accuracy    0.779 | loss    1.142\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  14 | time: 26.14s | valid accuracy    0.788 | valid loss    1.122 \n",
            "-----------------------------------------------------------\n",
            "| epoch  15 |    50/  110 batches | accuracy    0.778 | loss    1.134\n",
            "| epoch  15 |   100/  110 batches | accuracy    0.783 | loss    1.128\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  15 | time: 25.60s | valid accuracy    0.788 | valid loss    1.123 \n",
            "-----------------------------------------------------------\n",
            "| epoch  16 |    50/  110 batches | accuracy    0.780 | loss    1.118\n",
            "| epoch  16 |   100/  110 batches | accuracy    0.781 | loss    1.149\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  16 | time: 26.04s | valid accuracy    0.788 | valid loss    1.123 \n",
            "-----------------------------------------------------------\n",
            "| epoch  17 |    50/  110 batches | accuracy    0.781 | loss    1.133\n",
            "| epoch  17 |   100/  110 batches | accuracy    0.782 | loss    1.132\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  17 | time: 25.24s | valid accuracy    0.788 | valid loss    1.130 \n",
            "-----------------------------------------------------------\n",
            "| epoch  18 |    50/  110 batches | accuracy    0.781 | loss    1.131\n",
            "| epoch  18 |   100/  110 batches | accuracy    0.780 | loss    1.135\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  18 | time: 25.08s | valid accuracy    0.788 | valid loss    1.129 \n",
            "-----------------------------------------------------------\n",
            "| epoch  19 |    50/  110 batches | accuracy    0.778 | loss    1.126\n",
            "| epoch  19 |   100/  110 batches | accuracy    0.782 | loss    1.131\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  19 | time: 25.38s | valid accuracy    0.788 | valid loss    1.131 \n",
            "-----------------------------------------------------------\n",
            "| epoch  20 |    50/  110 batches | accuracy    0.782 | loss    1.124\n",
            "| epoch  20 |   100/  110 batches | accuracy    0.779 | loss    1.141\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  20 | time: 26.20s | valid accuracy    0.788 | valid loss    1.126 \n",
            "-----------------------------------------------------------\n",
            "| epoch  21 |    50/  110 batches | accuracy    0.781 | loss    1.134\n",
            "| epoch  21 |   100/  110 batches | accuracy    0.780 | loss    1.131\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  21 | time: 26.15s | valid accuracy    0.788 | valid loss    1.130 \n",
            "-----------------------------------------------------------\n",
            "| epoch  22 |    50/  110 batches | accuracy    0.779 | loss    1.132\n",
            "| epoch  22 |   100/  110 batches | accuracy    0.783 | loss    1.129\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  22 | time: 25.09s | valid accuracy    0.788 | valid loss    1.126 \n",
            "-----------------------------------------------------------\n",
            "| epoch  23 |    50/  110 batches | accuracy    0.783 | loss    1.118\n",
            "| epoch  23 |   100/  110 batches | accuracy    0.779 | loss    1.137\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  23 | time: 24.66s | valid accuracy    0.788 | valid loss    1.125 \n",
            "-----------------------------------------------------------\n",
            "| epoch  24 |    50/  110 batches | accuracy    0.780 | loss    1.136\n",
            "| epoch  24 |   100/  110 batches | accuracy    0.782 | loss    1.127\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  24 | time: 24.72s | valid accuracy    0.788 | valid loss    1.125 \n",
            "-----------------------------------------------------------\n",
            "| epoch  25 |    50/  110 batches | accuracy    0.781 | loss    1.137\n",
            "| epoch  25 |   100/  110 batches | accuracy    0.781 | loss    1.121\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  25 | time: 24.75s | valid accuracy    0.788 | valid loss    1.120 \n",
            "-----------------------------------------------------------\n",
            "Checking the results of test dataset.\n",
            "test accuracy    0.773 | test loss    1.149\n"
          ]
        }
      ],
      "source": [
        "from time import time\n",
        "import time\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dl, model, optimizer, criterion, epoch)\n",
        "    accu_val, loss_val = evaluate(valid_dl, model)\n",
        "    scheduler.step()\n",
        "    print(\"-\" * 59)\n",
        "    print(\n",
        "        \"| end of epoch {:3d} | time: {:5.2f}s \"\n",
        "        \"| valid accuracy {:8.3f} \"\n",
        "        \"| valid loss {:8.3f} \".format(\n",
        "            epoch,\n",
        "            time.time() - epoch_start_time,\n",
        "            accu_val,\n",
        "            loss_val\n",
        "        )\n",
        "    )\n",
        "    print(\"-\" * 59)\n",
        "\n",
        "print(\"Checking the results of test dataset.\")\n",
        "accu_test, loss_test = evaluate(test_dl, model)\n",
        "print(\"test accuracy {:8.3f} | test loss {:8.3f}\".format(accu_test, loss_test))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}