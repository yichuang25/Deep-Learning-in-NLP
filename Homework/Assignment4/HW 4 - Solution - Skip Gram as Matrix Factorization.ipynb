{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "CDkGUbEyetcF",
    "outputId": "d1cf5554-13c9-441b-c24b-b7bd965afecf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSYqe6vOeyI1"
   },
   "source": [
    "We will begin by making the model, which will be used later within the TF API. In this small example, we will be experimenting with different methods for random initialization of the embeddings, exploring how they may impact learning speed. So, we will use \"init_func\" as a placeholder for a generic initialization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-91EbJk3ev9w"
   },
   "outputs": [],
   "source": [
    "V = 5000\n",
    "E = 50\n",
    "\n",
    "class MFEmbedder(nn.Module):\n",
    "    def __init__(self, vsize, embdim):\n",
    "        super(MFEmbedder, self).__init__()\n",
    "        self.B = torch.nn.Parameter(torch.FloatTensor(vsize, embdim))\n",
    "        self.A = torch.nn.Parameter(torch.FloatTensor(vsize, embdim))\n",
    "        \n",
    "        nn.init.normal_(self.B)\n",
    "        nn.init.normal_(self.A)\n",
    "   \n",
    "    def forward(self, x=None):     \n",
    "        return torch.matmul(self.B, self.A.t())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MFEmbedder(V, E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ON0sflVee5sN"
   },
   "source": [
    "Now we need to make the special loss function that we learned to derive from last time. With this made, we will now be ready to learn SGNS vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UUVUgt9Ie2f8"
   },
   "outputs": [],
   "source": [
    "class SGNSLoss():\n",
    "    def __init__(self, Nij, Ni, Nj, N, K):\n",
    "        self.Nij = Nij\n",
    "        self.Ni = Ni\n",
    "        self.Nj = Nj\n",
    "        self.N = N\n",
    "        self.K = K\n",
    "        \n",
    "    def __call__(self, M_hat):\n",
    "        pos_samples = (self.Nij * nn.LogSigmoid()(M_hat)).sum()\n",
    "        \n",
    "        # This should be a 1 by 1 matrix.\n",
    "        neg_samples = (self.K / self.N) * torch.matmul(\n",
    "            torch.matmul(Nj.view(1, -1), nn.LogSigmoid()(-M_hat)),\n",
    "            self.Ni.view(-1, 1)\n",
    "        ).sum()\n",
    "        \n",
    "        return -(pos_samples + neg_samples) / self.N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f53utQP2e-sH"
   },
   "source": [
    "Now we should declare some of the variables that we want to work with. We are going to need to load one of my pre-made files that performs cooccurrence statistic extraction. Because this is a small example, we will just be using a 5000 word vocabulary so that we don't overload Google's cloud GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4ChTZkvre8jB"
   },
   "outputs": [],
   "source": [
    "## Boring data downloading stuff.\n",
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "def download_extract_zip(file_name):\n",
    "    with zipfile.ZipFile(file_name, 'r') as thezip:\n",
    "        for zipinfo in thezip.infolist():\n",
    "            with thezip.open(zipinfo) as thefile:\n",
    "                return pd.read_csv(\n",
    "                    thefile, \n",
    "                    sep=' ', \n",
    "                    header=None, \n",
    "                    names=['term', 'context', 'Nij']\n",
    "                )\n",
    "\n",
    "df = download_extract_zip('cooc.zip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>context</th>\n",
       "      <th>Nij</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chief</td>\n",
       "      <td>peace</td>\n",
       "      <td>2072.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>produces</td>\n",
       "      <td>venezuela</td>\n",
       "      <td>109.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resources</td>\n",
       "      <td>?</td>\n",
       "      <td>411.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>israelis</td>\n",
       "      <td>later</td>\n",
       "      <td>196.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>however</td>\n",
       "      <td>list</td>\n",
       "      <td>1395.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        term    context     Nij\n",
       "0      chief      peace  2072.8\n",
       "1   produces  venezuela   109.4\n",
       "2  resources          ?   411.2\n",
       "3   israelis      later   196.2\n",
       "4    however       list  1395.2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yzpcrDNJfB6P"
   },
   "outputs": [],
   "source": [
    "# Now we are loading in the data from the dataframe (quick and kind of dirty)\n",
    "vocab = {\"<unk>\": 0}\n",
    "invvocab = [\"<unk>\"]\n",
    "\n",
    "# count for words, not super efficient\n",
    "from collections import defaultdict\n",
    "counts = defaultdict(float)\n",
    "# for w in sorted(df['term'].unique()): ## alphabetically sorted\n",
    "#   counts[w] = 1\n",
    "for tup in df.itertuples(): ## frequency sorted\n",
    "    counts[tup.term] += tup.Nij\n",
    "\n",
    "for w in sorted(counts.keys(), key=lambda x: -counts[x]):\n",
    "    invvocab.append(w)\n",
    "    vocab[w] = len(vocab)\n",
    "\n",
    "V = len(vocab)\n",
    "\n",
    "# Given the vocabulary mapping above, we now fill in the Nij matrix.\n",
    "# NOTE - with large vocabs, this should be done using a *sparse* matrix!!!\n",
    "import numpy as np\n",
    "Nij_np = np.zeros((V, V))\n",
    "for tup in df.itertuples():\n",
    "    i = vocab[tup.term]\n",
    "    j = vocab[tup.context]\n",
    "    Nij_np[i, j] = tup.Nij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nij_np[0, 1], Nij_np[1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nij = torch.tensor(Nij_np, dtype=torch.float32)\n",
    "# marginalize to get the unigram counts\n",
    "N = Nij.sum()\n",
    "\n",
    "# col sums\n",
    "Ni = Nij.sum(axis=0)\n",
    "\n",
    "# row sum\n",
    "Nj = Nij.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-7.5978e+10, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Nij * nn.LogSigmoid()(model())).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.0059e+21, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(torch.matmul(\n",
    "    torch.matmul(\n",
    "        Nj.view(1, -1),\n",
    "        nn.LogSigmoid()(model())\n",
    "    ),\n",
    "    Ni.view(-1, 1)\n",
    ")).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(Nij.shape == torch.Size([V, V]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 5000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('i,j->ij', Ni, Nj).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "7zr8FsaNfDz9"
   },
   "outputs": [],
   "source": [
    "# marginalize to get the unigram counts\n",
    "N = Nij.sum()\n",
    "\n",
    "# col sums\n",
    "Ni = Nij.sum(axis=0)\n",
    "\n",
    "# row sum\n",
    "Nj = Nij.sum(axis=1)\n",
    "\n",
    "# Same thing.\n",
    "PMI_np_check = np.array(torch.log((N * Nij) / (torch.einsum('i,j->ij', Ni, Nj))))\n",
    "PMI_np = np.array(torch.log(\n",
    "    (N * Nij) / (Ni.repeat(V).view(V, -1).t() * Nj.repeat(V).view(V, -1))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PMI_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(torch.all(\n",
    "    torch.eq(\n",
    "        torch.einsum('i,j->ij', Ni, Nj),\n",
    "        Ni.repeat(V).view(V, -1).t() * Nj.repeat(V).view(V, -1)\n",
    ")).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PMI_np[:10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-yluEf3n_rN"
   },
   "source": [
    "Let's do a sanity check and examine the count statistics with some cool matplotlib visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "qTJ6-Urun-_G",
    "outputId": "abb1ab5d-380d-481c-e68b-5b37389eba72"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# investigate some of the statistics.\n",
    "flat_PMI_np = PMI_np.reshape(-1)\n",
    "pmi_negativeinf = np.sum(flat_PMI_np == -np.inf)\n",
    "total = np.prod(PMI_np.shape)\n",
    "print('Percent -infinities in the PMI matrix: {:.4f}%'.format(\n",
    "    100 * pmi_negativeinf / total))\n",
    "\n",
    "# visualize the matrix!\n",
    "_ = plt.figure()\n",
    "_ = plt.imshow(PMI_np, cmap=\"coolwarm\")\n",
    "_ = plt.xlabel(\"term index\")\n",
    "# _ = plt.ylabel(\"context index\")\n",
    "_ = plt.colorbar()\n",
    "# _ = plt.title(\"Visualization of the PMI matrix\")\n",
    "\n",
    "# turn the -infinities to something easier to work with\n",
    "PMI_np[PMI_np == -np.inf] = -4\n",
    "PMI_np[np.isnan(PMI_np)] = -4\n",
    "hist_pmis = flat_PMI_np[flat_PMI_np != -4]\n",
    "_ = plt.figure()\n",
    "n, bins, patches = plt.hist(hist_pmis, \n",
    "                            bins=100,              \n",
    "                            color=\"b\",\n",
    "                            alpha=1)\n",
    "print(\"Statistics of PMIs: \")\n",
    "print(\"{:0.4f} mean, {:0.4f} std\".format(np.mean(hist_pmis), \n",
    "                                         np.std(hist_pmis)))\n",
    "\n",
    "# Add shading to the histogram\n",
    "cm = plt.cm.get_cmap(\"coolwarm\")\n",
    "bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "col = bin_centers - min(bin_centers)\n",
    "col /= max(col)\n",
    "for c, p in zip(col, patches):\n",
    "    plt.setp(p, 'facecolor', cm(c))\n",
    "_ = plt.xlabel(\"PMI value\")\n",
    "_ = plt.ylabel(\"probability mass\")\n",
    "_ = plt.title(\"Histogram of PMIs (excluding -infinities)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "lXPxYsVkwiOV",
    "outputId": "a98c309b-fb1e-4502-f2c2-4a160028d08e"
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Note that there was a slight bug when extracting corpus statistics,\n",
    "since the context window (w=5) is symmetric, PMI(i,j) should always equal\n",
    "PMI(j,i); however, due to improper handling of context during the first 5 words\n",
    "of a document, the statistics are ever-so-slightly distorted.\n",
    "\"\"\"\n",
    "# look at the biggest PMIs\n",
    "sorted_ind = np.argsort(PMI_np, axis=None)\n",
    "\n",
    "print('--- Word pairs with the notable PMIs ---\\n')\n",
    "for i in range(1, 18):\n",
    "    ind = np.unravel_index(sorted_ind[-i], PMI_np.shape)\n",
    "    term, context = invvocab[ind[0]], invvocab[ind[1]]\n",
    "    print('{:8} {:14} (PMI = {:0.4f})'.format(term, context, float(PMI_np[ind])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the object representing the loss function.\n",
    "criterion = SGNSLoss(\n",
    "    Nij=Nij,\n",
    "    Ni=Ni,\n",
    "    Nj=Nj,\n",
    "    N=N,\n",
    "    k=1\n",
    ")\n",
    "\n",
    "# desired embedding dimensionality\n",
    "E = 50\n",
    "\n",
    "# Create the MF model!\n",
    "model = MFEmbedder(V, E)\n",
    "\n",
    "# Instantiate optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(model()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, n=1000):\n",
    "    \"Training loop for torch model.\"\n",
    "    losses = []\n",
    "    optimizer.zero_grad()\n",
    "    # This is B @ A.t().\n",
    "    M_hat = model()\n",
    "    loss = criterion(M_hat)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-M1u6d7lhX77"
   },
   "outputs": [],
   "source": [
    "# Grabs the most similar words to the current term. We scale the vectors and then do an inner product.\n",
    "def most_similar(term):\n",
    "    i = vocab[term]\n",
    "    emb = model.B[i]\n",
    "    embs = model.B.t() / torch.linalg.norm(model.B, axis=1)\n",
    "    cossims = torch.matmul(emb.view(1, -1), embs)\n",
    "    wordsims = torch.argsort(cossims, descending=True).view(-1)\n",
    "    return [invvocab[idx] for idx in wordsims[1:6]] # first is the word itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "qeFvqnruhVYy",
    "outputId": "955fe350-971b-4e2c-95c1-f01a9528f805"
   },
   "outputs": [],
   "source": [
    "# Now run a training loop! Note that these embeddings will be trained in less\n",
    "# than 15 minutes - much much much faster than it would take for the original \n",
    "# implementation. But, the speed of this is dependent on the vocabulary size.\n",
    "import time\n",
    "\n",
    "start = time.process_time() \n",
    "estart = time.process_time() \n",
    "results = []\n",
    "\n",
    "n_iters = 500 + 1\n",
    "print_every = 50\n",
    "\n",
    "for i in range(n_iters): \n",
    "    loss = train_step(model, optimizer)\n",
    "    results.append(loss)\n",
    "    if i % print_every == 0:\n",
    "        print('\\nstep {:4} - loss: {} ({:0.4f} seconds)'.format(\n",
    "            i, results[-1], time.process_time()  - estart)\n",
    "             )\n",
    "        print('\\t similar to \\\"money\\\": '+\" \".join(most_similar(\"money\")))\n",
    "        print('\\t similar to \\\"peace\\\": '+\" \".join(most_similar(\"peace\")))\n",
    "\n",
    "        estart = time.process_time() \n",
    "        \n",
    "print('\\nTotal time: {:0.4f} seconds.'.format(time.process_time()  - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "4a8pjCQQmA8S",
    "outputId": "154ebfd0-d964-4608-89fa-30a928784f4e"
   },
   "outputs": [],
   "source": [
    "# Let's look at the loss over time.\n",
    "from matplotlib import pyplot as plt\n",
    "x = np.array(list(range(len(results))))\n",
    "y = np.array(results)\n",
    "\n",
    "# use the underscore to avoid printing to colab\n",
    "_ = plt.figure()\n",
    "_ = plt.plot(x, y, '--r', label=\"loss\")\n",
    "_ = plt.xlabel(\"Iteration\")\n",
    "_ = plt.ylabel(\"Loss value\")\n",
    "# _ = plt.title(\"MF-SGNS loss over time\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_uJxoYpjVuP"
   },
   "source": [
    "Training is finished, and it looks like the model is learning to produce vectors with desirable semantic qualities! The loss is descreasing as well. As a final sanity check, let's do a manual inspection of a few more words, just to double check. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "ixnafiNliM6k",
    "outputId": "0452683e-5b3e-46d5-d251-06f90b820c50"
   },
   "outputs": [],
   "source": [
    "# manual qualitative inspection\n",
    "for w in [\"drive\", \"america\", \"east\", \"soviet\", \"belgium\", \"brussels\", \"1914\"]:\n",
    "    print(\"{:10}: {}\".format(w, \" \".join(most_similar(w))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irzyCs5sjzLq"
   },
   "source": [
    "As we see above, all of the outcomes are quite reasonable! E.g., 1914 (the year World War 1 started) is most similar to the next year of WW1, but is also highly similar to 1939 (the year WW2 started) -- very cool! We also observe that \"brussels\" is similar to other capitals, while \"belgium\" is similar to other countries - exactly what we would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "RAgGodCoX5sj",
    "outputId": "a7ebb1ca-5e95-4319-9940-bffea48282a6"
   },
   "outputs": [],
   "source": [
    "Mhat = model(None)\n",
    "_min, _max = -10, 10\n",
    "\n",
    "# Compare the original PMI matrix to our model's reproduction.\n",
    "f, axes = plt.subplots(1, 2, sharey=True, figsize=(12, 16))\n",
    "im1 = axes[0].imshow(PMI_np, cmap=\"coolwarm\", vmin = _min, vmax = _max)\n",
    "axes[0].set_title(r\"Original, PMI\")\n",
    "\n",
    "im2 = axes[1].imshow(Mhat, cmap=\"coolwarm\", vmin = _min, vmax = _max)\n",
    "axes[1].set_title(r\"Mhat (V @ W)\")\n",
    "f.colorbar(im2, ax=axes, orientation='horizontal', anchor=(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "id": "9Qz88R3-owyv",
    "outputId": "8fbe2dd2-3f13-4054-d92f-64817592be96"
   },
   "outputs": [],
   "source": [
    "# Compare the original PMI matrix to our model's reproduction.\n",
    "_min, _max = 0, 1\n",
    "f, axes = plt.subplots(1, 2, sharey=True, figsize=(12, 16))\n",
    "axes[0].imshow(tf.sigmoid(PMI_np), cmap=\"coolwarm\", vmin = _min, vmax = _max)\n",
    "axes[0].set_title(r\"Original, $\\sigma($PMI$)$\")\n",
    "\n",
    "Mhat = model(None)\n",
    "im2 = axes[1].imshow(tf.sigmoid(Mhat), cmap=\"coolwarm\", vmin = _min, vmax = _max)\n",
    "axes[1].set_title(r\"Mhat $\\sigma($V @ W$)$\")\n",
    "f.colorbar(im2, ax=axes, orientation='horizontal', anchor=(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1280
    },
    "id": "OAUpjCOqXCK1",
    "outputId": "5d16a443-9628-47f3-bab4-1a1d6c74236e"
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 2, sharey=False, figsize=(12, 16))\n",
    "\n",
    "hist_dots = tf.reshape(model(None), (-1,))\n",
    "hist_dots = tf.sigmoid(hist_dots)\n",
    "n, bins, patches = axes[0].hist(hist_dots, \n",
    "                                bins=100,              \n",
    "                                color=\"b\",\n",
    "                                alpha=1,\n",
    "                                key=\"Dot products\")\n",
    "axes[0].legend()\n",
    "\n",
    "n, bins, patches = axes[1].hist(tf.reshape(tf.sigmoid(PMI_np), (-1,)), \n",
    "                                bins=100,              \n",
    "                                color=\"b\",\n",
    "                                alpha=1,\n",
    "                                key=\"PMIs\")\n",
    "axes[1].legend()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
