{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97ab5757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd215dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64c0320b",
   "metadata": {},
   "source": [
    "### Get the data and process\n",
    "- This is the Mysterious island found in Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4e64a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length: 1130711\n",
      "Unique Characters: 85\n"
     ]
    }
   ],
   "source": [
    "## Reading and processing text\n",
    "with open('data/1268-0.txt', 'r', encoding=\"utf8\") as fp:\n",
    "    text=fp.read()\n",
    "    \n",
    "start_indx = text.find('THE MYSTERIOUS ISLAND')\n",
    "end_indx = text.find('End of the Project Gutenberg')\n",
    "\n",
    "text = text[start_indx:end_indx]\n",
    "char_set = set(text)\n",
    "print('Total Length:', len(text))\n",
    "print('Unique Characters:', len(char_set))\n",
    "assert(len(text) == 1130711)\n",
    "assert(len(char_set) == 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f650c1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76393bdb",
   "metadata": {},
   "source": [
    "### Tokenze and get other helpers\n",
    "- We do this manually since everything is character based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a445114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoded shape:  (1130711,)\n",
      "THE MYSTERIOUS       == Encoding ==>  [48 36 33  1 41 53 47 48 33 46 37 43 49 47  1]\n",
      "[37 47 40 29 42 32]  == Reverse  ==>  ISLAND\n"
     ]
    }
   ],
   "source": [
    "# The universe of words.\n",
    "chars_sorted = sorted(char_set)\n",
    "\n",
    "# Effectively, these maps are the tokenizer.\n",
    "char2int = {ch:i for i,ch in enumerate(chars_sorted)}\n",
    "int2char = np.array(chars_sorted)\n",
    "\n",
    "# Tokenize the entire corpus.\n",
    "text_encoded = np.array(\n",
    "    [char2int[ch] for ch in text],\n",
    "    dtype=np.int32)\n",
    "\n",
    "print('Text encoded shape: ', text_encoded.shape)\n",
    "\n",
    "print(text[:15], '     == Encoding ==> ', text_encoded[:15])\n",
    "print(text_encoded[15:21], ' == Reverse  ==> ', ''.join(int2char[text_encoded[15:21]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e0270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "720cd752",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2743a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text encoded shape:  (1130711,)\n",
      "THE MYSTERIOUS       == Encoding ==>  [48 36 33  1 41 53 47 48 33 46 37 43 49 47  1]\n",
      "[37 47 40 29 42 32]  == Reverse  ==>  ISLAND\n"
     ]
    }
   ],
   "source": [
    "print('Text encoded shape: ', text_encoded.shape)\n",
    "print(text[:15], '     == Encoding ==> ', text_encoded[:15])\n",
    "print(text_encoded[15:21], ' == Reverse  ==> ', ''.join(int2char[text_encoded[15:21]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d26f65c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(\n",
    "    np.array_equal(\n",
    "    text_encoded[:15],\n",
    "        [48, 36, 33, 1, 41, 53, 47, 48, 33, 46, 37, 43, 49, 47,  1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdcafe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c418ca0",
   "metadata": {},
   "source": [
    "### Process the data and get the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f429dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "\n",
    "text_chunks = [text_encoded[i:i+chunk_size] \n",
    "               for i in range(len(text_encoded)-chunk_size+1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e329fffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x8/2_vxppc52znb82mg86nv4y000000gp/T/ipykernel_28425/2801466430.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  seq_dataset = TextDataset(torch.tensor(text_chunks))\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text_chunk = self.text_chunks[idx]\n",
    "        return text_chunk[:-1].long(), text_chunk[1:].long()\n",
    "    \n",
    "seq_dataset = TextDataset(torch.tensor(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71328555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40]) torch.Size([40])\n",
      "Input (x): 'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTER'\n",
      "Target (y): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n",
      "\n",
      "torch.Size([40]) torch.Size([40])\n",
      "Input (x): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERI'\n",
      "Target (y): 'E MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nTHE MYSTERIO'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (seq, target) in enumerate(seq_dataset):\n",
    "    # 40 characters for source and target ...\n",
    "    print(seq.shape, target.shape)\n",
    "    print('Input (x):', repr(''.join(int2char[seq])))\n",
    "    print('Target (y):', repr(''.join(int2char[target])))\n",
    "    print()\n",
    "    if i == 1:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebb989c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a881b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "torch.manual_seed(1)\n",
    "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f77f7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45ed0b2f",
   "metadata": {},
   "source": [
    "### Write the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b4cbf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim) \n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = nn.LSTM(\n",
    "            embed_dim,\n",
    "            rnn_hidden_size, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, text, hidden=None, cell=None):\n",
    "        out = self.embedding(text)\n",
    "        \n",
    "        if hidden is not None:\n",
    "            out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        else:\n",
    "            out, (hidden, cell) = self.rnn(out)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, (hidden, cell)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        return hidden.to(device), cell.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c88a30b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can't jit trace a model with if-else.\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim) \n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = nn.LSTM(\n",
    "            embed_dim,\n",
    "            rnn_hidden_size, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, text, hidden, cell):\n",
    "        out = self.embedding(text)\n",
    "    \n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, (hidden, cell)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        return hidden.to(device), cell.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16c03dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00789dfd",
   "metadata": {},
   "source": [
    "### Do this right way - across all data all at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33380607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(85, 256)\n",
       "  (rnn): LSTM(256, 512, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=85, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(int2char)\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 512\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = RNN(vocab_size, embed_dim, rnn_hidden_size) \n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f47f48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 4.4364\n",
      "Epoch 100 loss: 1.7577\n",
      "Epoch 200 loss: 1.5690\n",
      "Epoch 300 loss: 1.4289\n",
      "Epoch 400 loss: 1.4803\n",
      "Epoch 500 loss: 1.3584\n",
      "Epoch 600 loss: 1.3528\n",
      "Epoch 700 loss: 1.3461\n",
      "Epoch 800 loss: 1.3206\n",
      "Epoch 900 loss: 1.2947\n",
      "Epoch 1000 loss: 1.2379\n",
      "Epoch 1100 loss: 1.2932\n",
      "Epoch 1200 loss: 1.2370\n",
      "Epoch 1300 loss: 1.3106\n",
      "Epoch 1400 loss: 1.2454\n",
      "Epoch 1500 loss: 1.2429\n",
      "Epoch 1600 loss: 1.2882\n",
      "Epoch 1700 loss: 1.2535\n",
      "Epoch 1800 loss: 1.1986\n",
      "Epoch 1900 loss: 1.1859\n",
      "Epoch 2000 loss: 1.2237\n",
      "Epoch 2100 loss: 1.2365\n",
      "Epoch 2200 loss: 1.2362\n",
      "Epoch 2300 loss: 1.2200\n",
      "Epoch 2400 loss: 1.2163\n",
      "Epoch 2500 loss: 1.1751\n",
      "Epoch 2600 loss: 1.1708\n",
      "Epoch 2700 loss: 1.2333\n",
      "Epoch 2800 loss: 1.1470\n",
      "Epoch 2900 loss: 1.1649\n",
      "Epoch 3000 loss: 1.2254\n",
      "Epoch 3100 loss: 1.1729\n",
      "Epoch 3200 loss: 1.1994\n",
      "Epoch 3300 loss: 1.1547\n",
      "Epoch 3400 loss: 1.1844\n",
      "Epoch 3500 loss: 1.1644\n",
      "Epoch 3600 loss: 1.2400\n",
      "Epoch 3700 loss: 1.1825\n",
      "Epoch 3800 loss: 1.1517\n",
      "Epoch 3900 loss: 1.1023\n",
      "Epoch 4000 loss: 1.1809\n",
      "Epoch 4100 loss: 1.2161\n",
      "Epoch 4200 loss: 1.1891\n",
      "Epoch 4300 loss: 1.1755\n",
      "Epoch 4400 loss: 1.1383\n",
      "Epoch 4500 loss: 1.1310\n",
      "Epoch 4600 loss: 1.1792\n",
      "Epoch 4700 loss: 1.1066\n",
      "Epoch 4800 loss: 1.1713\n",
      "Epoch 4900 loss: 1.1493\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "num_epochs = 5000 \n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    hidden, cell = model.init_hidden(batch_size)\n",
    "    \n",
    "    seq_batch, target_batch = next(iter(seq_dl))\n",
    "        \n",
    "    seq_batch = seq_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    # Specify these. This is the same as the first RNN, but this one you can trace.\n",
    "    hidden, cell = model.init_hidden(batch_size)\n",
    "\n",
    "    logits, _ = model(seq_batch, hidden, cell) \n",
    "    \n",
    "    loss += criterion(logits.view(logits.size(0) * logits.size(1), -1), target_batch.view(-1))\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    loss = loss.item()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch} loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec22f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4c59716",
   "metadata": {},
   "source": [
    "### Save the model - this is not needed but used for HW 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17af6f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(torch.jit.script(model), 'hw7_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03ab2df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, c = torch.zeros(1, batch_size, model.rnn_hidden_size), torch.zeros(1, batch_size, model.rnn_hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6925b65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.1M\thw7_model.pt\r\n"
     ]
    }
   ],
   "source": [
    "!du -h hw7_model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ffcc1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a3dc81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f398f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: [[0.01587624 0.11731043 0.86681336]]\n",
      "[[1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "logits = torch.tensor([[-1.0, 1.0, 3.0]])\n",
    "\n",
    "print('Probabilities:', nn.Softmax(dim=1)(logits).numpy())\n",
    "\n",
    "m = Categorical(logits=logits)\n",
    "samples = m.sample((10,))\n",
    " \n",
    "print(samples.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ec176d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0547467d",
   "metadata": {},
   "source": [
    "### Random decoding.\n",
    "- This compounds problems: once you make a mistake, you can't undo it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "614fb236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The island had been lips and royself with thirty feet high from the tenterly made by the sailor. “Shut it is twelve yes,” said Harding, climber the ladder and this time had occuen with his vessel from\n",
      "Reptiloads at the northern words, by Pencroft, and by means of libert, correct fancies, laid them. They would suff. There came were then coal, but which he promised to grow\n",
      "ever, he could have been kangaroosed. However, “if he thought to return to come alone he had very shelt?”\n",
      "\n",
      "“you arparation did not inclu\n"
     ]
    }
   ],
   "source": [
    "def random_sample(\n",
    "    model,\n",
    "    starting_str, \n",
    "    len_generated_text=500,\n",
    "    T = 1.0\n",
    "):\n",
    "\n",
    "    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
    "    \n",
    "    encoded_input = torch.reshape(encoded_input, (1, -1))\n",
    "\n",
    "    generated_str = starting_str\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    hidden, cell = model.init_hidden(1)\n",
    "    \n",
    "    hidden = hidden.to(device)\n",
    "    \n",
    "    cell = cell.to(device)\n",
    "        \n",
    "    # Build up the starting hidden and cell states.\n",
    "    # You can do this all in one go?\n",
    "    for c in range(len(starting_str)-1):\n",
    "        # Feed each letter 1 by 1 and then get the final hidden state.\n",
    "        out = encoded_input[:, c].reshape(1, 1)\n",
    "        _, (hidden, cell) = model(out, hidden, cell) \n",
    "    \n",
    "    last_char = encoded_input[:, -1]\n",
    "    for i in range(len_generated_text):\n",
    "        \n",
    "        logits, (hidden, cell) = model(last_char.reshape(1, 1), hidden, cell) \n",
    "        \n",
    "        logits = torch.squeeze(logits, 0)\n",
    "        \n",
    "        # Use temperature scaling here. For the HW, just do T = 1\n",
    "        m = Categorical(logits=logits / T)\n",
    "        \n",
    "        last_char = m.sample()\n",
    "        \n",
    "        generated_str += str(int2char[last_char])\n",
    "        \n",
    "    return generated_str\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model.to(device)\n",
    "print(greedy_sample(model, starting_str='The island'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f58492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0764392a",
   "metadata": {},
   "source": [
    "### Beam search algorithm.\n",
    "- Good article: https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28c6218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_sample(\n",
    "    model,\n",
    "    starting_str, \n",
    "    len_generated_text=500, \n",
    "    beams=5,\n",
    "    print_paths=True\n",
    "):\n",
    "\n",
    "    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
    "    \n",
    "    encoded_input = torch.reshape(encoded_input, (1, -1))\n",
    "\n",
    "    generated_str = starting_str\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    hidden, cell = model.init_hidden(1)\n",
    "    \n",
    "    hidden = hidden.to(device)\n",
    "    \n",
    "    cell = cell.to(device)\n",
    "        \n",
    "    # Build up the starting hidden and cell states.\n",
    "    # You can do this all in one go?\n",
    "    for i in range(len(starting_str)-1):\n",
    "        # Feed each letter 1 by 1 and then get the final hidden state.\n",
    "        out = encoded_input[:, i].reshape(1, 1)\n",
    "        _, (hidden, cell) = model(out, hidden, cell)\n",
    "    \n",
    "    beam_to_beam_data = {}\n",
    "    for beam in range(beams):\n",
    "        beam_to_beam_data[beam] = (hidden, cell, [char2int[generated_str[-1]]], generated_str, 0.0)\n",
    "        \n",
    "    for i in range(len_generated_text):\n",
    "        new_beams = []\n",
    "        \n",
    "        for beam in range(beams):\n",
    "            (hidden, cell, generated_char, generated_str, generated_log_prob) = beam_to_beam_data[beam]\n",
    "                        \n",
    "            last_char_int = torch.tensor(generated_char[-1]).reshape(1, 1)\n",
    "            \n",
    "            logits, (hidden, cell) = model(last_char_int, hidden, cell)\n",
    "                        \n",
    "            probs = nn.Softmax(dim=1)(logits.squeeze(1)).squeeze()\n",
    "                                                \n",
    "            for j, prob in enumerate(probs):\n",
    "                new_beams.append(\n",
    "                    (\n",
    "                        hidden,\n",
    "                        cell,\n",
    "                        generated_char + [j],\n",
    "                        generated_str + int2char[j],\n",
    "                        generated_log_prob + np.log(prob.item())\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "        new_beams_to_hidden_cell = {}\n",
    "        \n",
    "        for data in new_beams:\n",
    "            hidden, cell, generated_char, generated_str, generated_log_prob = data\n",
    "            new_beams_to_hidden_cell[\n",
    "                (generated_str, generated_log_prob)\n",
    "            ] = (hidden, cell, generated_char)\n",
    "            \n",
    "        new_beams = []\n",
    "            \n",
    "        for generated_str_generated_prob, hidden_cell_generated_char in new_beams_to_hidden_cell.items():\n",
    "            generated_str, generated_log_prob = generated_str_generated_prob\n",
    "            hidden, cell, generated_char = hidden_cell_generated_char\n",
    "            new_beams.append(\n",
    "                (hidden, cell, generated_char, generated_str, generated_log_prob)\n",
    "            )\n",
    "        \n",
    "        # Sort the beams from most proable to least. Use -log(p).\n",
    "        new_beams = sorted(new_beams, key = lambda beam_data: -beam_data[-1])\n",
    "                \n",
    "        # The number of beams considered should always satisfy this.\n",
    "        # Except for the first iteration.\n",
    "        print(\n",
    "            \"The number of beams is {}, the number of expected beams {} \".format(\n",
    "                len(new_beams), beams * len(char2int))\n",
    "        )\n",
    "        \n",
    "        if print_paths:\n",
    "            print(\"The first 5 paths beam paths and the associated data for them: \")\n",
    "            for beam in range(5):\n",
    "                generated_char, generated_str, generated_log_prob = new_beams[beam][2:]\n",
    "                print(\"Generated char indices: {} Generated Text: \\\"{}\\\" Generated Prob {:0.10f}\".format(\n",
    "                        generated_char[-7:], generated_str, np.exp(generated_log_prob)\n",
    "                ))\n",
    "            _ = input(\"Insert anything to continue ...\")\n",
    "                \n",
    "        \n",
    "        for beam in range(beams):\n",
    "            beam_to_beam_data[beam] = new_beams[beam]\n",
    "            \n",
    "        if print_paths:\n",
    "            print(\"Current beams considered: \")\n",
    "            for beam, beam_data in beam_to_beam_data.items():\n",
    "                print(beam, beam_data[-2])\n",
    "            print(\"\\n\")\n",
    "            \n",
    "    generated_strs = []\n",
    "    generated_chars = []\n",
    "    generated_log_probs = []\n",
    "        \n",
    "    for beam in range(beams):\n",
    "        (_, _, generated_char, generated_str, generated_log_prob) = beam_to_beam_data[beam]\n",
    "        generated_strs.append(generated_str)\n",
    "        generated_log_probs.append(generated_log_prob)\n",
    "        generated_chars.append(generated_char)\n",
    "        \n",
    "                \n",
    "    return generated_strs, generated_chars, [np.exp(_) for _ in generated_log_probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffe62bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of beams is 85, the number of expected beams 425 \n",
      "The first 5 paths beam paths and the associated data for them: \n",
      "Generated char indices: [58, 1] Generated Text: \"The island \" Generated Prob 0.4417397082\n",
      "Generated char indices: [58, 11] Generated Text: \"The island,\" Generated Prob 0.3666141331\n",
      "Generated char indices: [58, 28] Generated Text: \"The island?\" Generated Prob 0.0754827186\n",
      "Generated char indices: [58, 13] Generated Text: \"The island.\" Generated Prob 0.0583787486\n",
      "Generated char indices: [58, 2] Generated Text: \"The island!\" Generated Prob 0.0373276733\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "model.to('cpu')\n",
    "beams = 2\n",
    "\n",
    "generated_strs, generated_chars, generated_probs = beam_search_sample(model, starting_str=\"The island\", beams=5)\n",
    "\n",
    "for beam in range(beams):\n",
    "    print(f\"Beam {beam} information: \")\n",
    "    print(generated_strs[beam])\n",
    "    print(generated_probs[beam])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea2b1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
