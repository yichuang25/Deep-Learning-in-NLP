{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "945d9983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26874ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5610c9a7",
   "metadata": {},
   "source": [
    "Below are a few shorter problems. Please fill in the cells with your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "515100be",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILL_IN = \"FILL_IN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c57627",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "Given the example below, we have a bidirectional GRU. What is the connection between output and hidden? Explain in detail where exactly in output you can find hidden, and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d145620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = nn.GRU(1, 1, bidirectional=True, batch_first=True)\n",
    "\n",
    "# Use this same data for Problems 1 - 4\n",
    "x = torch.rand(4, 5, 1)\n",
    "\n",
    "# What is true about hidden and output? Where in output are the values in hidden? Be careful!\n",
    "output, hidden = gru(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01d3a5c",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "FILL IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0820ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "724b37a1",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "Consider the case when you have num_layers = 2 in a GRU as below. Describe what the conection now is between the hidden layer and the output layer. Specifically, what part of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "161ae3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = nn.GRU(1, 1, num_layers=2, batch_first=True)\n",
    "\n",
    "# What is true about hidden and output? Where in output are the values in hidden? Be careful!\n",
    "output, hidden = gru(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed258e9",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "FILL IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d75551b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6d08b6b",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "Given Problem 2, write code to get the representation across all time steps $T$ of the first layer. I.e., write code below to get $(\\overrightarrow{h}^{1}_1, \\ldots, \\overrightarrow{h}^{1}_T)$. Do this for a GRU with two layers. Note that \"output\" does not have what you want - you need to be a little clever to get this.\n",
    "\n",
    "\n",
    "Hint: See the bottom of this notebook if you are totally stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c77f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = nn.GRU(1, 1, num_layers=2, batch_first=True)\n",
    "\n",
    "# What is true about hidden and output? Where in output are the values in hidden? Be careful!\n",
    "output, hidden = gru(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a2c11cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FILL_IN'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer\n",
    "# One easy way to do this is to do this is manually. Just have two GRUs and hae one's output feed into the other.\n",
    "# Then, loow through the named parameters of the gru and insert them into one or the other of the two grus above.\n",
    "\n",
    "FILL_IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eb6049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af8d7f4e",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "\n",
    "In this problem we want to deal with sequences that are not the same length. Suppose we have 3 sequences of data $a, b, c$, where the length of $a, b$ and $c$ are $2, 3$ and $4$ respectively. Assume you want to do a batch operation where the batch consists of $a, b$ and $c$ and you want to run these through the model. At the end, you'd like to get the final hidden state for each sentence. One way to do this is to pad all the sequences so they are length 4 and feed the 3 by 4 vector into the GRU.\n",
    "- What is the problem with doing this? What is ineffcient about it? What is inefficint about output_padded and how it was computed?\n",
    "\n",
    "Investigate how to do this better using the 4 imports below. You may not need all of these functions.\n",
    "I.e. create a batch of size 3 containing the 3 tensors.\n",
    "\n",
    "- What is output_padded1 vs output_padded? Compare the shape and the values inside. What is better about the way output_padded2 was computed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01e97171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_sequence, pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Each tensor is in (length, values) format\n",
    "a = torch.randn(2, 1)\n",
    "b = torch.randn(3, 1)\n",
    "c = torch.randn(4, 1)\n",
    "\n",
    "la, lb, lc = 2, 3, 4\n",
    "\n",
    "rnn = nn.GRU(1, 1, num_layers=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f3f335d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Use pad_sequence to pass the create a batch of size 3 and pad it so each sequence has length 4\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Use batch_first=True\u001b[39;00m\n\u001b[1;32m      9\u001b[0m padded \u001b[38;5;241m=\u001b[39m FILL_IN\n\u001b[0;32m---> 11\u001b[0m output_padded, hidden_padded \u001b[38;5;241m=\u001b[39m \u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(padded\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Use pack_padded_sequence to pack a, b and c\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Use batch_first=True\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mlenv/lib/python3.8/site-packages/torch/nn/modules/rnn.py:926\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    925\u001b[0m     batch_sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m     is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    927\u001b[0m     batch_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "# Answer:\n",
    "# One easy way to do this is to do this is manually. Just have two GRUs and hae one's output feed into the other\n",
    "# Then, low through the named parameters of the gru and insert them into one or the other of the two grus above\n",
    "\n",
    "seq = [a, b, c]\n",
    "\n",
    "# Use pad_sequence to pass the create a batch of size 3 and pad it so each sequence has length 4\n",
    "# Use batch_first=True\n",
    "padded = FILL_IN\n",
    "\n",
    "output_padded, hidden_padded = gru(padded)\n",
    "\n",
    "print(padded.shape)\n",
    "\n",
    "# Use pack_padded_sequence to pack a, b and c\n",
    "# Use batch_first=True\n",
    "packed1 = FILL_IN\n",
    "# pack_padded_sequence is older, the below is a newer command\n",
    "packed2 = FILL_IN\n",
    "\n",
    "output_packed1, hidden_packed1 = gru(packed1)\n",
    "output_packed2, hidden_packed2 = gru(packed2)\n",
    "\n",
    "# Use pad_packed_sequence to unpack the results above; you now get padded results, similar to the ouput_padded and hidden_padded above\n",
    "# What is different and the same about output_padded1 and output_padded?\n",
    "# Why is it more efficient to use this method as opposed to just pad all elements in a batch and pass them through?\n",
    "output_padded1, output_lengths1 = FILL_IN\n",
    "output_padded2, output_lengths12= FILL_IN\n",
    "\n",
    "print(output_padded2.shape)\n",
    "\n",
    "assert(torch.all(torch.eq(output_padded1, output_padded2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1068749c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d391d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69625d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "446b5fc0",
   "metadata": {},
   "source": [
    "This example is like the previous one in HW 9, but now we want a more complicated model with attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c935ce54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bb58c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3d0d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf61f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba853a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "# Only use pairs where the english data (pair[1]) has the prefix above\n",
    "# Also, only consider data where pair[0] and pair[1] have length less than MAX_LENGTH\n",
    "# \"length\" here means the number of tokens, you need to split pair[0] and pair[1] on ' ' then get the length\n",
    "def filterPair(p):\n",
    "    return FILL_IN\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d34d3fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4345\n",
      "eng 2803\n",
      "['tu es vraiment tres productif aujourd hui .', 'you are really very productive today .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "958787f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = FILL_IN\n",
    "        # Make the encoder a GRU and also make it bidirectional.\n",
    "        # Let it have 1 layers in the vertical direction.\n",
    "        self.gru = FILL_IN\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # Get the embeddings and reshape to be (1, 1, -1)\n",
    "        # Why? remember we use batch size = 1 in this HW for simplicity\n",
    "        embedded = FILL_IN\n",
    "        output = embedded\n",
    "        output, hidden = FILL_IN\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c75043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        \n",
    "        # H\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # vocab_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Intialize the embedding going from vocab_size to H\n",
    "        self.embedding = FILL_IN\n",
    "        \n",
    "        # Initialize the attention projection as a Linear layer from 2*H to self.max_length\n",
    "        self.attention_projection = FILL_IN\n",
    "        \n",
    "        # Initialize the output projection as a Linear layer from 2*H -> H (this is before we project to the vocab_size)\n",
    "        self.output_projection = FILL_IN\n",
    "        \n",
    "        # Intialize a Dropout layer with self.dropout_p probability\n",
    "        self.dropout = FILL_IN\n",
    "        \n",
    "        # Make the GRU be unidirectional and also with 1 hidden layer\n",
    "        # Input and hidden data each have a dimension of H\n",
    "        self.gru = FILL_IN\n",
    "        \n",
    "        # Intialize a Linear layer going from H to vocab_size\n",
    "        self.out = FILL_IN\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # (1, 1, H)\n",
    "        embedded = FILL_IN\n",
    "        \n",
    "        # Pass embedding through the dropout layer\n",
    "        embedded = FILL_IN\n",
    "\n",
    "        # (1, 2*H)\n",
    "        # Concatenate yt and kt to get a vector (y_t, k_{t-1})\n",
    "        embedded_hidden = torch.cat((embedded[0], hidden[0]), 1)\n",
    "        \n",
    "        # (1, MAX_LENGTH)\n",
    "        # Project the above vector to get a vector mixing the elements of the above\n",
    "        # This vector will be used to get attention scores with all the encoder embeddings\n",
    "        # Here, the scores are scores = W_a[y_{t}, k_{t-1}] + b_a where W_a an b_a are in self.attention_projection\n",
    "        # You can have other formats here, but the one above is enough for this problem\n",
    "        attention_scores = FILL_IN\n",
    "        \n",
    "        # (1, MAX_LENGTH)\n",
    "        # Get the attention weights from the scores\n",
    "        # I.e. get probabilistic from the above scores\n",
    "        attention_weights = FILL_IN\n",
    "        \n",
    "        # (1, 1, H)\n",
    "        # Multiply the weights by the hidden states (h_1, h_2, .., h_{T_x}) of the encoder\n",
    "        # This should be a vector of the above dimensions, so you'll need unsqueeze\n",
    "        # One way to do this is using torch.bmm on these unsqueezed vectors\n",
    "        # This will be the at vector that mixed the encoder's hidden representations; \"c_{t}\"\" in lecture\n",
    "        attention_context = FILL_IN\n",
    "\n",
    "        # (1, 2*H)\n",
    "        # Concatenate (yt, at) to get a vector that we will use to predict the output\n",
    "        output = FILL_IN\n",
    "        \n",
    "        # (1, 1, H)\n",
    "        # Project the above vector into a new vector we'll use to predict with\n",
    "        # unsqueeze(0) the result to get the right dimensions\n",
    "        output = FILL_IN\n",
    "\n",
    "        # (1, H)\n",
    "        # Pass through ReLU\n",
    "        output = FILL_IN\n",
    "        \n",
    "        # (1, H) and (1, H)\n",
    "        # Pass the output and hidden through the GRU. Note that we apply attention before we pass into the GRU\n",
    "        # The input (\"output\" vector) has attentional information in it\n",
    "        output, hidden = FILL_IN\n",
    "\n",
    "        # (1, vocab_size)\n",
    "        # Either apply log_softmax to output or leave it alone\n",
    "        # This will have you use the NLLLoss or the CrossEntropyLoss\n",
    "        output = FILL_IN\n",
    "        return output, hidden, attention_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4c517ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a sentence by ' ' and return a list of the tokens (int ids) for each word\n",
    "# Use word2index\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return FILL_IN\n",
    "\n",
    "# Call the above on a sentence\n",
    "# After calling, add the EOS_token (int id) to the gotten list\n",
    "# Return a tensor, but reshape it so it's dimensions (-1, 1)\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = FILL_IN\n",
    "    FILL_IN\n",
    "    return FILL_IN\n",
    "\n",
    "# For a source, target pair, call the above. Return a tuple of 2 tensors, one input_tensor and another an output_tensor\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = FILL_IN\n",
    "    target_tensor = FILL_IN\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b62e21b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    # Initialize the hidden states\n",
    "    encoder_hidden = FILL_IN\n",
    "\n",
    "    # Reset the optimizer gradients to 0\n",
    "    FILL_IN\n",
    "    FILL_IN\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    # Initialize the encoder outputs - these are used to store the vector's we'll use to get attention scores\n",
    "    # This should be (max_length, H) and all zeros to start\n",
    "    encoder_outputs = FILL_IN\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # Pass the data through the encoder\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = FILL_IN\n",
    "        # Save the encoder output into \"encoder_outputs\"\n",
    "        encoder_outputs[ei] = FILL_IN\n",
    "\n",
    "    # Initialize the decoder input to the SOS_token\n",
    "    decoder_input = FILL_IN\n",
    "\n",
    "    # Initialize the hidden states of the decoder with the hidden states of the encoder\n",
    "    decoder_hidden = FILL_IN\n",
    "\n",
    "    # For this pair, use teacher forcing with 50% probability, else don't\n",
    "    use_teacher_forcing = FILL_IN\n",
    "    \n",
    "    target_length_used = 0\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        \n",
    "        target_length_used = target_length\n",
    "        \n",
    "        for di in range(target_length):\n",
    "            # Push decoder_input, decoder_hidden, and decoder_cell through the decoder\n",
    "            decoder_output, decoder_hidden, decoder_attention = FILL_IN\n",
    "            loss += FILL_IN\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            # Push decoder_input, decoder_hidden, and decoder_cell through the decoder\n",
    "            decoder_output, decoder_hidden, decoder_attention = FILL_IN\n",
    "            # Get greedy top probability prediction\n",
    "            topv, topi = FILL_IN\n",
    "            decoder_input = FILL_IN  # detach from history as input\n",
    "            \n",
    "            # Get the loss\n",
    "            loss += FILL_IN\n",
    "            \n",
    "            # Update the target_length_used\n",
    "            target_length_used += FILL_IN\n",
    "            \n",
    "            # If the EOS_token was generated, exit\n",
    "            FILL_IN\n",
    "\n",
    "    # Collect gradients\n",
    "    FILL_IN\n",
    "\n",
    "    # Do a step; do this both for the encoder and the decoder\n",
    "    FILL_IN\n",
    "    FILL_IN\n",
    "\n",
    "    return loss.item() / target_length_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a066a924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d194e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # This locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd09179c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b7adca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    # Initialize the encoder and decoder optimizers with the above learning rate\n",
    "    encoder_optimizer = FILL_IN\n",
    "    decoder_optimizer = FILL_IN\n",
    "    \n",
    "    # Get n_iters training pairs\n",
    "    # In this example, we are effectively doing SGD with batch size 1\n",
    "    training_pairs = FILL_IN\n",
    "    \n",
    "    # The loss; either NLLLoss if you use log sigmoids or CrossEntropyLoss if you use logits\n",
    "    criterion = FILL_IN\n",
    "\n",
    "    for it in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[it - 1]\n",
    "        input_tensor = FILL_IN\n",
    "        target_tensor = FILL_IN\n",
    "\n",
    "        # Train on the input, target pair\n",
    "        loss = FILL_IN\n",
    "        \n",
    "        # Update the total loss and the plot loss\n",
    "        # We can plot and print at different granularities\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if it % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, it / n_iters),\n",
    "                                         it, it / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if it % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "            showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc25e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3fdef8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x8/2_vxppc52znb82mg86nv4y000000gp/T/ipykernel_22830/4031975464.py:25: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 6s (- 15m 36s) (5000 6%) 2.8469\n",
      "2m 12s (- 14m 23s) (10000 13%) 2.2902\n",
      "3m 20s (- 13m 20s) (15000 20%) 1.9997\n",
      "4m 27s (- 12m 16s) (20000 26%) 1.7155\n",
      "5m 35s (- 11m 11s) (25000 33%) 1.5362\n",
      "6m 44s (- 10m 6s) (30000 40%) 1.3786\n",
      "7m 52s (- 9m 0s) (35000 46%) 1.2275\n",
      "9m 1s (- 7m 54s) (40000 53%) 1.1052\n",
      "10m 10s (- 6m 47s) (45000 60%) 1.0113\n",
      "11m 19s (- 5m 39s) (50000 66%) 0.9068\n",
      "12m 32s (- 4m 33s) (55000 73%) 0.8100\n",
      "13m 42s (- 3m 25s) (60000 80%) 0.7271\n",
      "14m 51s (- 2m 17s) (65000 86%) 0.6697\n",
      "16m 0s (- 1m 8s) (70000 93%) 0.6021\n",
      "17m 12s (- 0m 0s) (75000 100%) 0.5668\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttentionDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "trainIters(encoder, decoder, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dc5c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef5f5df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75d9c8a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_LENGTH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(encoder, decoder, sentence, max_length\u001b[38;5;241m=\u001b[39m\u001b[43mMAX_LENGTH\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;66;03m# Transform the input sentence into a tensor\u001b[39;00m\n\u001b[1;32m      4\u001b[0m         input_tensor \u001b[38;5;241m=\u001b[39m tensorFromSentence(FILL_IN)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MAX_LENGTH' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        # Transform the input sentence into a tensor\n",
    "        input_tensor = tensorFromSentence(FILL_IN)\n",
    "        \n",
    "        input_length = input_tensor.size()[0]\n",
    "        \n",
    "        # Initilize the hidden and cell states of the LSTM\n",
    "        encoder_hidden = FILL_IN\n",
    "\n",
    "        # Initialize the encoder outputs as in train\n",
    "        encoder_outputs = FILL_IN\n",
    "\n",
    "        # Run the data through the LSTM word by word manually\n",
    "        # At each step, feed in the input, the hidden state, and the cell state and calture the new hidden / cell states\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = FILL_IN\n",
    "            encoder_outputs[ei] = FILL_IN\n",
    "\n",
    "        # Initialize the decoder input with a SOS_token\n",
    "        decoder_input = FILL_IN  # SOS\n",
    "\n",
    "        # Initialize the decoder hidden state with the encoder's hidden state\n",
    "        decoder_hidden = FILL_IN\n",
    "\n",
    "        # Initialize the decoded words and a matrix of T by T length which will store the attention weights\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            # Pass the data through the decoder\n",
    "            decoder_output, decoder_hidden, decoder_attention = FILL_IN\n",
    "            # Save the attention matrix above - you might want to look at this later to debug\n",
    "            decoder_attentions[di] = FIL_IN\n",
    "            # Get the top (1) decoder output as use this as the next input\n",
    "            topv, topi = FILL_IN\n",
    "            \n",
    "            # Add the word for the topi token to the decoded_words\n",
    "            FILL_IN\n",
    "            \n",
    "            # If EOS was decoded, break\n",
    "            FILL_IN           \n",
    "\n",
    "            # Save the token above as the next input\n",
    "            decoder_input = FILL_IN\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da86763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f363235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=7500, debug=False):\n",
    "    bleu_scores = []\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        if debug:\n",
    "            print('French Original: ', pair[0])\n",
    "            print('English Reference: ', pair[1])\n",
    "        # Leave out the EOS symbol\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0])\n",
    "        \n",
    "        # If EOS is at the end, remove it from output_words\n",
    "        FILL_IN\n",
    "                            \n",
    "        output_sentence = ' '.join(output_words)\n",
    "        # Use pair[1] as te refernce and get the BLEU score based on just 2 grams with 50% weight each\n",
    "        score = FILL_IN\n",
    "        \n",
    "        # Append the BLEU score to the list of BLEU scores\n",
    "        FILL_IN\n",
    "        if debug:\n",
    "            print('Candidate Translation: ', output_sentence)\n",
    "            print('BLEU: ', score)\n",
    "            print('')\n",
    "    print('The mean BLEU score is: ', np.mean(bleu_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cff40e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7abe4887",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluateRandomly' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluateRandomly\u001b[49m(encoder, decoder)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluateRandomly' is not defined"
     ]
    }
   ],
   "source": [
    "# You should get something > 60 % here\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b861c69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d5402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "394d4732",
   "metadata": {},
   "source": [
    "Hint for Problem 3: create two layer=1 GRU models and transfer the 2 layer's model's parameters to the appropriate GRU. Then, manually push data through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c646c2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you follow the hint, you need 2 GRU models each of 1 layer, gru1 and gru2\n",
    "# hidden1, output1 is the output of gru1 if you push x through\n",
    "# hidden 2, output2 is the output and hidden state of gru2 if you push output1 through\n",
    "# These asserts below should pass\n",
    "# You need to transfer the gru model's appropriate parameters to the right model, gru1 or gru2, then manuall pass data through\n",
    "assert(torch.all(torch.eq(output, output2)))\n",
    "assert(torch.all(torch.eq(hidden, torch.vstack((hidden1, hidden2)))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
